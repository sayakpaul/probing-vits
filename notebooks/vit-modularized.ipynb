{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vit-cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPCt0AIw09kwRjCdPhNjpue"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install the tensorflow-addons library\n",
        "! pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeylnyhO0XFI",
        "outputId": "e151a4a1-d086-45e3-b0a1-3f2116a3a3d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "ECpw87uekiYJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V11PE9fjkc11"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "Woud0UQCkuGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA\n",
        "BATCH_SIZE = 256\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (32, 32, 3)\n",
        "\n",
        "# AUGMENTATION\n",
        "IMAGE_SIZE = 32\n",
        "PATCH_SIZE = 8  # Size of the patches to be extracted from the input images.\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "\n",
        "# ViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 128\n",
        "NUM_HEADS = 4\n",
        "NUM_LAYERS = 4\n",
        "MLP_UNITS = [\n",
        "    PROJECTION_DIM * 4,\n",
        "    PROJECTION_DIM,\n",
        "]\n",
        "DROPOUT_RATE = 0.0"
      ],
      "metadata": {
        "id": "rvQUc1mxkrd7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer blocks\n",
        "\n",
        "References:\n",
        "\n",
        "- https://github.com/huggingface/transformers/blob/master/src/transformers/models/vit_mae/modeling_vit_mae.py\n",
        "- https://github.com/google-research/vision_transformer/blob/main/vit_jax/models.py\n",
        "- https://keras.io/examples/vision/image_classification_with_vision_transformer/"
      ],
      "metadata": {
        "id": "8Vu2eI5YlIcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbedding(keras.layers.Layer):\n",
        "    \"\"\"Positional embedding layer.\n",
        "\n",
        "    Args:\n",
        "        pos_mode (`string`): Either \"sin-cos\" or \"learn\".\n",
        "        embed_dim (`int`): The dimensions for embedding.\n",
        "        num_patches (`int`): The number of patches of the image.\n",
        "        add_cls_token (`boolean`): Whether class token is added or not.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        pos_mode,\n",
        "        embed_dim,\n",
        "        num_patches,\n",
        "        add_cls_token,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pos_mode = pos_mode\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_patches = num_patches\n",
        "        self.add_cls_token = add_cls_token\n",
        "        \n",
        "        # Compute the positions\n",
        "        positions = self.num_patches\n",
        "        positions += 1 if self.add_cls_token else 0\n",
        "        \n",
        "        # Build the range of patches\n",
        "        self.pos_flat_patches = tf.range(positions, dtype=tf.float32, delta=1)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Encode the positions with an Embedding layer.\n",
        "        if self.pos_mode == \"learn\":\n",
        "            self.pos_embedding = layers.Embedding(\n",
        "                input_dim=self.num_patches,\n",
        "                output_dim=self.embed_dim,\n",
        "                embeddings_initializer=keras.initializers.RandomNormal(\n",
        "                    stddev=0.02\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"pos_mode\": self.pos_mode,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_patches\": self.num_patches,\n",
        "            \"add_cls_token\": self.add_cls_token,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_1d_sincos_pos_embed(self):\n",
        "        # Build the sine-cosine positional embedding.\n",
        "        omega = tf.range(self.embed_dim // 2, dtype=\"float32\")\n",
        "        omega /= self.embed_dim / 2.0\n",
        "        omega = 1.0 / 10000 ** omega  # (D/2,)\n",
        "\n",
        "        out = tf.einsum(\"m,d->md\", self.pos_flat_patches, omega)  # (M, D/2), outer product\n",
        "\n",
        "        emb_sin = tf.sin(out)  # (M, D/2)\n",
        "        emb_cos = tf.cos(out)  # (M, D/2)\n",
        "\n",
        "        emb = tf.concat([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "        return emb\n",
        "\n",
        "    def get_learnable_pos_embed(self):\n",
        "        emb = self.pos_embedding(self.pos_flat_patches)\n",
        "\n",
        "        return emb\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.pos_mode == \"learn\":\n",
        "            pos_emb = self.get_learnable_pos_embed()\n",
        "        else:\n",
        "            pos_emb = self.get_1d_sincos_pos_embed()\n",
        "        \n",
        "        # Inject the positional embeddings with the tokens\n",
        "        inputs = inputs + pos_emb\n",
        "        return inputs"
      ],
      "metadata": {
        "id": "M3G9czHllc2I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        layer_norm_eps,\n",
        "        num_heads,\n",
        "        projection_dim,\n",
        "        dropout_rate,\n",
        "        mlp_hidden_dim,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.num_heads = num_heads\n",
        "        self.projection_dim = projection_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.mlp_hidden_dim = mlp_hidden_dim\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.layer_norm1 = layers.LayerNormalization(epsilon=self.layer_norm_eps)\n",
        "        self.mhsa = layers.MultiHeadAttention(\n",
        "            num_heads=self.num_heads, key_dim=self.projection_dim, dropout=self.dropout_rate\n",
        "        )\n",
        "        self.dropout = layers.Dropout(self.dropout_rate)\n",
        "        self.add = layers.Add()\n",
        "        self.layer_norm2 = layers.LayerNormalization(epsilon=self.layer_norm_eps)\n",
        "        self.mlp = self.build_mlp()\n",
        "    \n",
        "    def build_mlp(self):\n",
        "        # Iterate over the hidden units and\n",
        "        # add Dense => Dropout.\n",
        "        mlp_layers = list()\n",
        "        for idx, units in enumerate(self.mlp_hidden_dim):\n",
        "            mlp_layers.append(\n",
        "                layers.Dense(\n",
        "                    units,\n",
        "                    activation=tf.nn.gelu if idx == 0 else None,\n",
        "                    kernel_initializer=\"glorot_uniform\",\n",
        "                    bias_initializer=keras.initializers.RandomNormal(stddev=1e-6),\n",
        "                )\n",
        "            )\n",
        "            mlp_layers.append(\n",
        "                layers.Dropout(self.dropout_rate)\n",
        "            )\n",
        "        \n",
        "        mlp = keras.Sequential(mlp_layers)\n",
        "        return mlp\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"layer_norm_eps\": self.layer_norm_eps,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"projection_dim\": self.projection_dim,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "            \"mlp_hidden_dim\": self.mlp_hidden_dim,\n",
        "        })\n",
        "        return config\n",
        "    \n",
        "    def call(self, encoded_patches):\n",
        "        # Layer normalization 1.\n",
        "        x1 = self.layer_norm1(encoded_patches)\n",
        "\n",
        "        # Multi Head Self Attention layer 1.\n",
        "        attention_output = self.mhsa(x1, x1)\n",
        "        attention_output = self.dropout(attention_output)\n",
        "\n",
        "        # Skip connection 1.\n",
        "        x2 = self.add([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer normalization 2.\n",
        "        x3 = self.layer_norm2(x2)\n",
        "\n",
        "        # MLP layer 1.\n",
        "        x4 = self.mlp(x3)\n",
        "\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = self.add([x2, x4])\n",
        "        return encoded_patches"
      ],
      "metadata": {
        "id": "lxa1T8Q61chp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        pos_mode,\n",
        "        embed_dim,\n",
        "        num_patches,\n",
        "        add_cls_token,\n",
        "        patch_size,\n",
        "        batch_size,\n",
        "        num_layers,\n",
        "        layer_norm_eps,\n",
        "        num_heads,\n",
        "        dropout_rate,\n",
        "        mlp_hidden_dim,\n",
        "        is_gap,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pos_mode = pos_mode\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_patches = num_patches\n",
        "        self.add_cls_token = add_cls_token\n",
        "        self.patch_size = patch_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.mlp_hidden_dim = mlp_hidden_dim\n",
        "        self.is_gap = is_gap\n",
        "    \n",
        "    def build(self, input_shapes):\n",
        "        # Build the patchification module\n",
        "        self.patchify = keras.Sequential([\n",
        "            layers.Conv2D(\n",
        "                filters=self.embed_dim,\n",
        "                kernel_size=self.patch_size,\n",
        "                strides=self.patch_size,\n",
        "                padding=\"VALID\",\n",
        "            ),\n",
        "            layers.Reshape(target_shape=(-1, self.embed_dim))    \n",
        "        ])\n",
        "        \n",
        "        # Build the postional embedding module\n",
        "        self.position_embedding = PositionEmbedding(\n",
        "            pos_mode=self.pos_mode,\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_patches=self.num_patches,\n",
        "            add_cls_token=self.add_cls_token\n",
        "        )\n",
        "        \n",
        "        # Build the transformer encoder module\n",
        "        self.transformer_layers = self.build_transformer()\n",
        "        \n",
        "        # If class token needs to be added, then build the class token\n",
        "        if self.add_cls_token:\n",
        "            initial_value = tf.zeros((1, 1, self.embed_dim))\n",
        "            self.class_token = tf.Variable(\n",
        "                initial_value=initial_value, trainable=True\n",
        "            )\n",
        "        \n",
        "        # If GAP is used, build the GAP layer\n",
        "        if self.is_gap:\n",
        "            self.gap = layers.GlobalAveragePooling1D(keepdims=True)\n",
        "        \n",
        "    def build_transformer(self):\n",
        "        self.transformer_layers = list()\n",
        "        for _ in range(self.num_layers):\n",
        "            self.transformer_layers.append(\n",
        "                TransformerBlock(\n",
        "                    layer_norm_eps=self.layer_norm_eps,\n",
        "                    num_heads=self.num_heads,\n",
        "                    projection_dim=self.embed_dim,\n",
        "                    dropout_rate=self.dropout_rate,\n",
        "                    mlp_hidden_dim=self.mlp_hidden_dim,\n",
        "                )\n",
        "            )\n",
        "        return self.transformer_layers\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"pos_mode\": self.pos_mode,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_patches\": self.num_patches,\n",
        "            \"add_cls_token\": self.add_cls_token,\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"num_layers\": self.num_layers,\n",
        "            \"layer_norm_eps\": self.layer_norm_eps,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "            \"mlp_hidden_dim\": self.mlp_hidden_dim,\n",
        "            \"is_gap\": self.is_gap,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, images):\n",
        "        # Get the patches and linearly project them\n",
        "        projected_patches = self.patchify(images)\n",
        "\n",
        "        # Add the class token if add_class_token is True\n",
        "        if self.add_cls_token:\n",
        "            class_token = tf.tile(self.class_token, (self.batch_size, 1, 1))\n",
        "            projected_patches = tf.concat([class_token, projected_patches], axis=1)\n",
        "\n",
        "        # Inject the positional embedding to the projected patches\n",
        "        encoded_patches = self.position_embedding(projected_patches)\n",
        "\n",
        "        x = encoded_patches\n",
        "        # Pass through the transfomer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # compute the global represenatation vector\n",
        "        if self.is_gap:\n",
        "            x = self.gap(x)\n",
        "        else:\n",
        "            x = x[:, 0:1]\n",
        "        \n",
        "        # Return the logits\n",
        "        return x"
      ],
      "metadata": {
        "id": "yoK-h_IU0dbb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = tf.random.normal((BATCH_SIZE, ) + INPUT_SHAPE)\n",
        "\n",
        "vit = ViT(\n",
        "    pos_mode=\"learn\",\n",
        "    embed_dim=PROJECTION_DIM,\n",
        "    num_patches=NUM_PATCHES,\n",
        "    add_cls_token=True,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    mlp_hidden_dim=MLP_UNITS,\n",
        "    is_gap=True,\n",
        ")\n",
        "\n",
        "logits = vit(images)\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K74HumH5Fkf5",
        "outputId": "538dc02c-664b-478c-a91d-7b1b094b1243"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 1, 128)\n"
          ]
        }
      ]
    }
  ]
}