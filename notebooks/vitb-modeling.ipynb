{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vitb-modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow-addons -qq\n",
        "! pip install ml-collections -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeylnyhO0XFI",
        "outputId": "02c4012d-d916-4ae1-908c-824f8adc2f95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1 MB 14.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 4.9 MB/s \n",
            "\u001b[?25h  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "ECpw87uekiYJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V11PE9fjkc11"
      },
      "outputs": [],
      "source": [
        "import ml_collections\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration\n",
        "\n",
        "References:\n",
        "\n",
        "- https://github.com/google-research/vision_transformer/blob/main/vit_jax/configs/models.py#L103"
      ],
      "metadata": {
        "id": "Woud0UQCkuGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_config() -> ml_collections.ConfigDict:\n",
        "    config = ml_collections.ConfigDict()\n",
        "\n",
        "    config.batch_size = 32\n",
        "    config.input_shape = (224, 224, 3)\n",
        "\n",
        "    config.image_size = 224\n",
        "    config.patch_size = 16\n",
        "    config.num_patches = (config.image_size // config.patch_size) ** 2\n",
        "    config.num_classes = 10\n",
        "\n",
        "    config.pos_emb_mode = \"sin-cos\"\n",
        "\n",
        "    config.layer_norm_eps = 1e-6\n",
        "    config.projection_dim = 768\n",
        "    config.num_heads = 12\n",
        "    config.num_layers = 12\n",
        "    config.mlp_units = [\n",
        "        config.projection_dim * 4,\n",
        "        config.projection_dim,\n",
        "    ]\n",
        "    config.dropout_rate = 0.0\n",
        "    config.classifier = \"token\"\n",
        "\n",
        "    return config.lock()"
      ],
      "metadata": {
        "id": "rvQUc1mxkrd7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer blocks\n",
        "\n",
        "References:\n",
        "\n",
        "- https://github.com/huggingface/transformers/blob/master/src/transformers/models/vit_mae/modeling_vit_mae.py\n",
        "- https://github.com/google-research/vision_transformer/blob/main/vit_jax/models.py\n",
        "- https://keras.io/examples/vision/image_classification_with_vision_transformer/"
      ],
      "metadata": {
        "id": "8Vu2eI5YlIcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, config: ml_collections.ConfigDict, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.config = config\n",
        "        \n",
        "        # Compute the positions\n",
        "        positions = self.config.num_patches\n",
        "        positions += 1 if self.config.classifier == \"token\" else 0\n",
        "        \n",
        "        # Build the sequence of positions in 1D\n",
        "        self.pos_flat_patches = tf.range(positions, dtype=tf.float32, delta=1)\n",
        "\n",
        "        # Encode the positions with an Embedding layer.\n",
        "        if self.config.pos_emb_mode == \"learn\":\n",
        "            self.pos_embedding = layers.Embedding(\n",
        "                input_dim=self.config.num_patches,\n",
        "                output_dim=self.config.projection_dim,\n",
        "                embeddings_initializer=keras.initializers.RandomNormal(stddev=0.02),\n",
        "            )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(self.config)\n",
        "        return config\n",
        "\n",
        "    def get_1d_sincos_pos_embed(self):\n",
        "        # Build the sine-cosine positional embedding.\n",
        "        omega = tf.range(self.config.projection_dim // 2, dtype=tf.float32)\n",
        "        omega /= self.config.projection_dim / 2.0\n",
        "        omega = 1.0 / 10000 ** omega  # (D/2,)\n",
        "\n",
        "        out = tf.einsum(\"m,d->md\", self.pos_flat_patches, omega)  # (M, D/2), outer product\n",
        "\n",
        "        emb_sin = tf.sin(out)  # (M, D/2)\n",
        "        emb_cos = tf.cos(out)  # (M, D/2)\n",
        "\n",
        "        emb = tf.concat([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "        return emb\n",
        "\n",
        "    def get_learnable_pos_embed(self):\n",
        "        emb = self.pos_embedding(self.pos_flat_patches)\n",
        "        return emb\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.config.pos_emb_mode == \"learn\":\n",
        "            pos_emb = self.get_learnable_pos_embed()\n",
        "        else:\n",
        "            pos_emb = self.get_1d_sincos_pos_embed()\n",
        "        \n",
        "        # Inject the positional embeddings with the tokens\n",
        "        outputs = inputs + pos_emb\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "M3G9czHllc2I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, dropout_rate, hidden_units):\n",
        "    # Iterate over the hidden units and\n",
        "    # add Dense => Dropout.\n",
        "    for idx, units in enumerate(hidden_units):\n",
        "        x = layers.Dense(\n",
        "            units,\n",
        "            activation=tf.nn.gelu if idx == 0 else None,\n",
        "            kernel_initializer=\"glorot_uniform\",\n",
        "            bias_initializer=keras.initializers.RandomNormal(stddev=1e-6),\n",
        "        )(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ueo1-NzqXCzv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(config, name):\n",
        "    num_patches = (\n",
        "        config.num_patches + 1\n",
        "        if config.classifier == \"token\"\n",
        "        else config.num_patches + 0\n",
        "    )\n",
        "    encoded_patches = layers.Input((num_patches, config.projection_dim))\n",
        "\n",
        "    # Layer normalization 1.\n",
        "    x1 = layers.LayerNormalization(epsilon=config.layer_norm_eps)(encoded_patches)\n",
        "\n",
        "    # Multi Head Self Attention layer 1.\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=config.num_heads,\n",
        "        key_dim=config.projection_dim,\n",
        "        dropout=config.dropout_rate,\n",
        "    )(x1, x1)\n",
        "    attention_output = layers.Dropout(config.dropout_rate)(attention_output)\n",
        "\n",
        "    # Skip connection 1.\n",
        "    x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "    # Layer normalization 2.\n",
        "    x3 = layers.LayerNormalization(epsilon=config.layer_norm_eps)(x2)\n",
        "\n",
        "    # MLP layer 1.\n",
        "    x4 = mlp(x3, hidden_units=config.mlp_units, dropout_rate=config.dropout_rate)\n",
        "\n",
        "    # Skip connection 2.\n",
        "    outputs = layers.Add()([x2, x4])\n",
        "\n",
        "    return keras.Model(encoded_patches, outputs, name=name)"
      ],
      "metadata": {
        "id": "In0uA0VxXFgV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTClassifier(keras.Model):\n",
        "    def __init__(self, config: ml_collections.ConfigDict, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.config = config\n",
        "\n",
        "        self.projection = layers.Conv2D(\n",
        "            filters=config.projection_dim,\n",
        "            kernel_size=(config.patch_size, config.patch_size),\n",
        "            strides=(config.patch_size, config.patch_size),\n",
        "            padding=\"VALID\",\n",
        "            name=\"projection\",\n",
        "        )\n",
        "\n",
        "        self.positional_embedding = PositionalEmbedding(\n",
        "            config, name=\"positional_embedding\"\n",
        "        )\n",
        "        self.transformer_blocks = [\n",
        "            transformer(config, name=f\"transformer_block_{i}\")\n",
        "            for i in range(config.num_layers)\n",
        "        ]\n",
        "\n",
        "        if config.classifier == \"token\":\n",
        "            initial_value = tf.zeros((1, 1, config.projection_dim))\n",
        "            self.cls_token = tf.Variable(\n",
        "                initial_value=initial_value, trainable=True, name=\"cls\"\n",
        "            )\n",
        "\n",
        "        if config.classifier == \"gap\":\n",
        "            self.gap_layer = layers.GlobalAvgPool1D()\n",
        "\n",
        "        self.dropout = layers.Dropout(config.dropout_rate)\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=config.layer_norm_eps)\n",
        "        self.classifier_head = layers.Dense(\n",
        "            config.num_classes, kernel_initializer=\"zeros\", name=\"classifier\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Create patches and project the pathces.\n",
        "        projected_patches = self.projection(inputs)\n",
        "        n, h, w, c = projected_patches.shape\n",
        "        projected_patches = tf.reshape(projected_patches, [n, h * w, c])\n",
        "\n",
        "        # Append class token if needed.\n",
        "        if self.config.classifier == \"token\":\n",
        "            cls_token = tf.tile(self.cls_token, (n, 1, 1))\n",
        "            projected_patches = tf.concat([cls_token, projected_patches], axis=1)\n",
        "\n",
        "        # Add positional embeddings to the projected patches.\n",
        "        encoded_patches = self.positional_embedding(\n",
        "            projected_patches\n",
        "        )  # (B, number_patches, projection_dim)\n",
        "        encoded_patches = self.dropout(encoded_patches)\n",
        "\n",
        "        # Iterate over the number of layers and stack up blocks of\n",
        "        # Transformer.\n",
        "        for transformer_module in self.transformer_blocks:\n",
        "            # Add a Transformer block.\n",
        "            encoded_patches = transformer_module(encoded_patches)\n",
        "\n",
        "        # Final layer normalization.\n",
        "        representation = self.layer_norm(encoded_patches)\n",
        "\n",
        "        # Pool representation.\n",
        "        if self.config.classifier == \"token\":\n",
        "            encoded_patches = representation[:, 0]\n",
        "        elif self.config.classifier == \"gap\":\n",
        "            encoded_patches = self.gap_layer(representation)\n",
        "\n",
        "        # Classification head.\n",
        "        output = self.classifier_head(encoded_patches)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "dBh-Hde8XbMR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verification\n",
        "\n",
        "| Name | Classifier | Pos Embed Mode |\n",
        "| :-- | :--: | :--: |\n",
        "| vit_classifier | `CLS TOKEN` | `sin-cos` |\n",
        "| vit_classifier_pos_learn | `CLS TOKEN` | `learn` |\n",
        "| vit_classifier_w_gap | `GAP` | `sin-cos` |\n",
        "| vit_classifier_w_gap_pos_learn | `GAP` | `learn` |"
      ],
      "metadata": {
        "id": "9wnw7YPjX6dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit_b16_config = get_config()"
      ],
      "metadata": {
        "id": "LSydEouCX6BP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"classifier: {vit_b16_config.classifier}\\npos_emb_mode: {vit_b16_config.pos_emb_mode}\")\n",
        "vit_classifier = ViTClassifier(vit_b16_config, name=\"vit_cls_token_pos_sincos\")\n",
        "random_logits = vit_classifier(tf.ones((10, 224, 224, 3)))\n",
        "random_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-SWAKUxX9HQ",
        "outputId": "8354a78c-cc7e-442a-f4e6-3fe2b4faf43d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier: token\n",
            "pos_emb_mode: sin-cos\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with vit_b16_config.unlocked():\n",
        "    vit_b16_config.pos_emb_mode = \"learn\"\n",
        "\n",
        "print(f\"classifier: {vit_b16_config.classifier}\\npos_emb_mode: {vit_b16_config.pos_emb_mode}\")\n",
        "\n",
        "vit_classifier_pos_learn = ViTClassifier(vit_b16_config, name=\"vit_cls_token_pos_learn\")\n",
        "random_logits = vit_classifier_pos_learn(tf.ones((10, 224, 224, 3)))\n",
        "random_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWHn08pXYiQR",
        "outputId": "0d428d37-a56d-47af-fec4-2c1484674825"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier: token\n",
            "pos_emb_mode: learn\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with vit_b16_config.unlocked():\n",
        "    vit_b16_config.pos_emb_mode = \"sin-cos\"\n",
        "    vit_b16_config.classifier = \"gap\"\n",
        "\n",
        "print(f\"classifier: {vit_b16_config.classifier}\\npos_emb_mode: {vit_b16_config.pos_emb_mode}\")\n",
        "vit_classifier_w_gap = ViTClassifier(vit_b16_config, name=\"vit_with_gap_pos_sincos\")\n",
        "random_logits = vit_classifier_w_gap(tf.ones((10, 224, 224, 3)))\n",
        "random_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPNfvDePYBr5",
        "outputId": "79dcf335-67e6-4bd7-c827-dd8c06a08e83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier: gap\n",
            "pos_emb_mode: sin-cos\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with vit_b16_config.unlocked():\n",
        "    vit_b16_config.pos_emb_mode = \"learn\"\n",
        "    vit_b16_config.classifier = \"gap\"\n",
        "\n",
        "print(f\"classifier: {vit_b16_config.classifier}\\npos_emb_mode: {vit_b16_config.pos_emb_mode}\")\n",
        "vit_classifier_w_gap_pos_learn = ViTClassifier(vit_b16_config, name=\"vit_with_gap_pos_learn\")\n",
        "random_logits = vit_classifier_w_gap_pos_learn(tf.ones((10, 224, 224, 3)))\n",
        "random_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRVVMpLeYZXI",
        "outputId": "c7343b0d-a2aa-4f23-c023-a31af69baa99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier: gap\n",
            "pos_emb_mode: learn\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer Inspection"
      ],
      "metadata": {
        "id": "niZGUwRDgXXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit_classifier_w_gap.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wzqZsYJgPE9",
        "outputId": "f10702d7-6f57-429b-d013-71d70999bb4a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vit_with_gap_pos_sincos\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " projection (Conv2D)         multiple                  590592    \n",
            "                                                                 \n",
            " positional_embedding (Posit  multiple                 0         \n",
            " ionalEmbedding)                                                 \n",
            "                                                                 \n",
            " transformer_block_0 (Functi  (None, 196, 768)         33065472  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " transformer_block_1 (Functi  (None, 196, 768)         33065472  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " transformer_block_2 (Functi  (None, 196, 768)         33065472  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " transformer_block_3 (Functi  (None, 196, 768)         33065472  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " transformer_block_4 (Functi  (None, 196, 768)         33065472  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " transformer_block_5 (Functi  (None, 196, 768)         33065472  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " transformer_block_6 (Functi  (None, 196, 768)         33065472  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " transformer_block_7 (Functi  (None, 196, 768)         33065472  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " transformer_block_8 (Functi  (None, 196, 768)         33065472  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " transformer_block_9 (Functi  (None, 196, 768)         33065472  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " transformer_block_10 (Funct  (None, 196, 768)         33065472  \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " transformer_block_11 (Funct  (None, 196, 768)         33065472  \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " global_average_pooling1d (G  multiple                 0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_110 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            " layer_normalization_74 (Lay  multiple                 1536      \n",
            " erNormalization)                                                \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  7690      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 397,385,482\n",
            "Trainable params: 397,385,482\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit_classifier_w_gap.get_layer(\"transformer_block_3\").summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzWMyar7gbX8",
        "outputId": "5227ef0a-98fc-42ef-9c3a-80275f89b04b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer_block_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_28 (InputLayer)          [(None, 196, 768)]   0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_56 (LayerN  (None, 196, 768)    1536        ['input_28[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_27 (Multi  (None, 196, 768)    28339968    ['layer_normalization_56[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_83 (Dropout)           (None, 196, 768)     0           ['multi_head_attention_27[0][0]']\n",
            "                                                                                                  \n",
            " add_54 (Add)                   (None, 196, 768)     0           ['dropout_83[0][0]',             \n",
            "                                                                  'input_28[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_57 (LayerN  (None, 196, 768)    1536        ['add_54[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_54 (Dense)               (None, 196, 3072)    2362368     ['layer_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_84 (Dropout)           (None, 196, 3072)    0           ['dense_54[0][0]']               \n",
            "                                                                                                  \n",
            " dense_55 (Dense)               (None, 196, 768)     2360064     ['dropout_84[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_85 (Dropout)           (None, 196, 768)     0           ['dense_55[0][0]']               \n",
            "                                                                                                  \n",
            " add_55 (Add)                   (None, 196, 768)     0           ['add_54[0][0]',                 \n",
            "                                                                  'dropout_85[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 33,065,472\n",
            "Trainable params: 33,065,472\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}