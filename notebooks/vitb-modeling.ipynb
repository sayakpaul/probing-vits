{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hb5GAuOB4fcF",
   "metadata": {
    "id": "hb5GAuOB4fcF"
   },
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83246ac9",
   "metadata": {
    "id": "83246ac9"
   },
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uYmjA0OY4hlV",
   "metadata": {
    "id": "uYmjA0OY4hlV"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "References:\n",
    "\n",
    "* https://github.com/google-research/vision_transformer/blob/main/vit_jax/configs/models.py#L103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a50d1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.batch_size = 32\n",
    "    config.input_shape = (224, 224, 3)\n",
    "\n",
    "    config.image_size = 224\n",
    "    config.patch_size = 16\n",
    "    config.num_patches = (config.image_size // config.patch_size) ** 2\n",
    "    config.num_classes = 10\n",
    "    \n",
    "    config.layer_norm_eps = 1e-6\n",
    "    config.projection_dim = 768\n",
    "    config.num_heads = 12\n",
    "    config.num_layers = 12\n",
    "    config.mlp_units = [\n",
    "        config.projection_dim * 4,\n",
    "        config.projection_dim,\n",
    "    ]\n",
    "    config.dropout_rate = 0.0\n",
    "    config.classifier = \"token\"\n",
    "\n",
    "    return config.lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XXaYwenB4jCZ",
   "metadata": {
    "id": "XXaYwenB4jCZ"
   },
   "source": [
    "## Vision Transformer blocks\n",
    "\n",
    "References:\n",
    "\n",
    "* https://github.com/google-research/vision_transformer/blob/main/vit_jax/models.py\n",
    "* https://keras.io/examples/vision/image_classification_with_vision_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdafdf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        \n",
    "        self.num_patches = (\n",
    "            self.config.num_patches + 1\n",
    "            if self.config.classifier == \"token\"\n",
    "            else self.config.num_patches + 0\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "\n",
    "        self.embedding = layers.Embedding(\n",
    "            input_dim=self.num_patches,\n",
    "            output_dim=self.config.projection_dim,\n",
    "            embeddings_initializer=keras.initializers.RandomNormal(stddev=0.02),\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoded_positions = self.embedding(self.positions)\n",
    "        return inputs + encoded_positions\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(self.config)\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce45e69e",
   "metadata": {
    "id": "ce45e69e"
   },
   "outputs": [],
   "source": [
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    # Iterate over the hidden units and\n",
    "    # add Dense => Dropout.\n",
    "    for idx, units in enumerate(hidden_units):\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.gelu if idx == 0 else None,\n",
    "            kernel_initializer=\"glorot_uniform\",\n",
    "            bias_initializer=keras.initializers.RandomNormal(stddev=1e-6),\n",
    "        )(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35352365",
   "metadata": {
    "id": "35352365"
   },
   "outputs": [],
   "source": [
    "def transformer(config, name):\n",
    "    num_patches = (\n",
    "        config.num_patches + 1\n",
    "        if config.classifier == \"token\"\n",
    "        else config.num_patches + 0\n",
    "    )\n",
    "    encoded_patches = layers.Input((num_patches, config.projection_dim))\n",
    "\n",
    "    # Layer normalization 1.\n",
    "    x1 = layers.LayerNormalization(epsilon=config.layer_norm_eps)(encoded_patches)\n",
    "\n",
    "    # Multi Head Self Attention layer 1.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.num_heads,\n",
    "        key_dim=config.projection_dim,\n",
    "        dropout=config.dropout_rate,\n",
    "    )(x1, x1)\n",
    "    attention_output = layers.Dropout(config.dropout_rate)(attention_output)\n",
    "\n",
    "    # Skip connection 1.\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "    # Layer normalization 2.\n",
    "    x3 = layers.LayerNormalization(epsilon=config.layer_norm_eps)(x2)\n",
    "\n",
    "    # MLP layer 1.\n",
    "    x4 = mlp(x3, hidden_units=config.mlp_units, dropout_rate=config.dropout_rate)\n",
    "\n",
    "    # Skip connection 2.\n",
    "    outputs = layers.Add()([x2, x4])\n",
    "\n",
    "    return keras.Model(encoded_patches, outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd92458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTClassifier(keras.Model):\n",
    "    def __init__(self, config: ml_collections.ConfigDict, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "\n",
    "        self.projection = layers.Conv2D(\n",
    "            filters=config.projection_dim,\n",
    "            kernel_size=(config.patch_size, config.patch_size),\n",
    "            strides=(config.patch_size, config.patch_size),\n",
    "            padding=\"VALID\",\n",
    "            name=\"projection\",\n",
    "        )\n",
    "\n",
    "        self.positional_embedding = PositionalEmbedding(\n",
    "            config, name=\"positional_embedding\"\n",
    "        )\n",
    "        self.transformer_blocks = [\n",
    "            transformer(config, name=f\"transformer_block_{i}\")\n",
    "            for i in range(config.num_layers)\n",
    "        ]\n",
    "\n",
    "        if config.classifier == \"token\":\n",
    "            initial_value = tf.zeros((1, 1, config.projection_dim))\n",
    "            self.cls_token = tf.Variable(\n",
    "                initial_value=initial_value, trainable=True, name=\"cls\"\n",
    "            )\n",
    "\n",
    "        if config.classifier == \"gap\":\n",
    "            self.gap_layer = layers.GlobalAvgPool1D()\n",
    "\n",
    "        self.dropout = layers.Dropout(config.dropout_rate)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=config.layer_norm_eps)\n",
    "        self.classifier_head = layers.Dense(\n",
    "            config.num_classes, kernel_initializer=\"zeros\", name=\"classifier\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Create patches and project the pathces.\n",
    "        projected_patches = self.projection(inputs)\n",
    "        n, h, w, c = projected_patches.shape\n",
    "        projected_patches = tf.reshape(projected_patches, [n, h * w, c])\n",
    "\n",
    "        # Append class token if needed.\n",
    "        if self.config.classifier == \"token\":\n",
    "            cls_token = tf.tile(self.cls_token, (n, 1, 1))\n",
    "            projected_patches = tf.concat([cls_token, projected_patches], axis=1)\n",
    "\n",
    "        # Add positional embeddings to the projected patches.\n",
    "        encoded_patches = self.positional_embedding(\n",
    "            projected_patches\n",
    "        )  # (B, number_patches, projection_dim)\n",
    "        encoded_patches = self.dropout(encoded_patches)\n",
    "\n",
    "        # Iterate over the number of layers and stack up blocks of\n",
    "        # Transformer.\n",
    "        for transformer_module in self.transformer_blocks:\n",
    "            # Add a Transformer block.\n",
    "            encoded_patches = transformer_module(encoded_patches)\n",
    "\n",
    "        # Final layer normalization.\n",
    "        representation = self.layer_norm(encoded_patches)\n",
    "\n",
    "        # Pool representation.\n",
    "        if self.config.classifier == \"token\":\n",
    "            encoded_patches = representation[:, 0]\n",
    "        elif self.config.classifier == \"gap\":\n",
    "            encoded_patches = self.gap_layer(representation)\n",
    "\n",
    "        # Classification head.\n",
    "        output = self.classifier_head(encoded_patches)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49465a1c",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348116cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_b16_config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9467578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 10:52:50.065208: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier = ViTClassifier(vit_b16_config, name=\"vit_with_cls_token\")\n",
    "random_logits = vit_classifier(tf.ones((10, 224, 224, 3)))\n",
    "random_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd47f3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with vit_b16_config.unlocked():\n",
    "    vit_b16_config.classifier = \"gap\"\n",
    "\n",
    "vit_classifier_w_gap = ViTClassifier(vit_b16_config, name=\"vit_with_gap\")\n",
    "vit_classifier_w_gap(tf.ones((10, 224, 224, 3))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0d181",
   "metadata": {},
   "source": [
    "## Layer inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca1d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vit_with_gap\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " projection (Conv2D)         multiple                  590592    \n",
      "                                                                 \n",
      " positional_embedding (Posit  multiple                 150528    \n",
      " ionalEmbedding)                                                 \n",
      "                                                                 \n",
      " transformer_block_0 (Functi  (None, 196, 768)         33065472  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " transformer_block_1 (Functi  (None, 196, 768)         33065472  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " transformer_block_2 (Functi  (None, 196, 768)         33065472  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " transformer_block_3 (Functi  (None, 196, 768)         33065472  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " transformer_block_4 (Functi  (None, 196, 768)         33065472  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " transformer_block_5 (Functi  (None, 196, 768)         33065472  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " transformer_block_6 (Functi  (None, 196, 768)         33065472  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " transformer_block_7 (Functi  (None, 196, 768)         33065472  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " transformer_block_8 (Functi  (None, 196, 768)         33065472  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " transformer_block_9 (Functi  (None, 196, 768)         33065472  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " transformer_block_10 (Funct  (None, 196, 768)         33065472  \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " transformer_block_11 (Funct  (None, 196, 768)         33065472  \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling1d (G  multiple                 0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " layer_normalization_49 (Lay  multiple                 1536      \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  7690      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 397,536,010\n",
      "Trainable params: 397,536,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_classifier_w_gap.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af4856b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_block_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 196, 768)]   0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 196, 768)    1536        ['input_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 196, 768)    28339968    ['layer_normalization_31[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_46 (Dropout)           (None, 196, 768)     0           ['multi_head_attention_15[0][0]']\n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 196, 768)     0           ['dropout_46[0][0]',             \n",
      "                                                                  'input_16[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, 196, 768)    1536        ['add_30[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 196, 3072)    2362368     ['layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_47 (Dropout)           (None, 196, 3072)    0           ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 196, 768)     2360064     ['dropout_47[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_48 (Dropout)           (None, 196, 768)     0           ['dense_31[0][0]']               \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 196, 768)     0           ['add_30[0][0]',                 \n",
      "                                                                  'dropout_48[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33,065,472\n",
      "Trainable params: 33,065,472\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_classifier_w_gap.get_layer(\"transformer_block_3\").summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "data2vec-vision.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
