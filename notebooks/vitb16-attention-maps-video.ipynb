{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is identical to `dino-attention-maps-video.ipynb` but it generates attention heatmaps using a supervised pre-trained ViT-B16 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJetL6lX2g9G",
    "outputId": "5f33b07e-45a9-4494-c671-015d15aa2dd9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ml_collections transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMOFmCzj078-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pformat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the master dataframe from [AugReg paper](https://arxiv.org/abs/2106.10270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "L3hxjjGg1cPq",
    "outputId": "93d65983-f5ba-4d5d-aec1-3220d7874ead"
   },
   "outputs": [],
   "source": [
    "with tf.io.gfile.GFile(\"gs://vit_models/augreg/index.csv\") as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a checkpoint\n",
    "\n",
    "**Criteria**\n",
    "\n",
    "* B16 architecture\n",
    "* Resolution 224\n",
    "* Pacth size 16\n",
    "* Supervised pre-training on ImageNet-1k\n",
    "* Best top-1 accuracy on ImageNet-1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "wmEbN88h1gIB",
    "outputId": "98891903-698d-4fa5-adbc-e956c0698801"
   },
   "outputs": [],
   "source": [
    "b16s = df.query(\n",
    "    'ds==\"i1k\" & adapt_resolution==224 & adapt_ds==\"imagenet2012\" & name==\"B/16\"'\n",
    ").sort_values(\"adapt_final_test\", ascending=False)\n",
    "b16s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "haNrB4U41mzx",
    "outputId": "0947ce5d-f280-4fd2-cf58-ed3f154248bf"
   },
   "outputs": [],
   "source": [
    "best_b16_i1k_checkpoint = str(b16s.iloc[0][\"adapt_filename\"])\n",
    "b16s.iloc[0][\"adapt_filename\"], b16s.iloc[0][\"adapt_final_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_-ogt7s2Br7",
    "outputId": "114a748b-b2bb-4a5b-ba32-d9c94be73a99"
   },
   "outputs": [],
   "source": [
    "filename = best_b16_i1k_checkpoint\n",
    "\n",
    "path = f\"gs://vit_models/augreg/{filename}.npz\"\n",
    "\n",
    "print(f\"{tf.io.gfile.stat(path).length / 1024 / 1024:.1f} MiB - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy over the checkpoint and load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "gmEQQ_AG2DcP",
    "outputId": "130eb792-4904-4865-9175-55949d1523b2"
   },
   "outputs": [],
   "source": [
    "!gsutil cp {path} .\n",
    "local_path = path.split(\"//\")[-1].split(\"/\")[-1]\n",
    "local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Opr9tcc2PXm"
   },
   "outputs": [],
   "source": [
    "with open(local_path, \"rb\") as f:\n",
    "    params_jax = np.load(f)\n",
    "    params_jax = dict(zip(params_jax.keys(), params_jax.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the model in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqc-fWx72SXt"
   },
   "outputs": [],
   "source": [
    "!git clone -q https://github.com/sayakpaul/deit-tf -b new-block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMYe-i_r2YC2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"deit-tf\")\n",
    "\n",
    "from vit.vit_models import ViTClassifier\n",
    "from vit.model_configs import base_config\n",
    "from utils import helpers\n",
    "from vit.layers import mha\n",
    "\n",
    "from transformers.tf_utils import shape_list\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRICWiYO2vJZ"
   },
   "outputs": [],
   "source": [
    "class ViTB16Extended(ViTClassifier):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def interpolate_pos(self, x, N, h, w):\n",
    "        class_pos_embed = self.positional_embedding[:, 0]\n",
    "        patch_pos_embed = self.positional_embedding[:, 1:]\n",
    "        dim = shape_list(x)[-1]\n",
    "\n",
    "        # Calculate the resolution to which we need to perform interpolation.\n",
    "        h0 = h // self.config.patch_size\n",
    "        w0 = w // self.config.patch_size\n",
    "\n",
    "        # We avoided the following because UpSampling2D won't support float sizes:\n",
    "        # https://github.com/facebookresearch/dino/blob/main/vision_transformer.py#L186\n",
    "        sqrt_N = tf.math.sqrt(tf.cast(N, \"float32\"))\n",
    "        sqrt_N_ceil = tf.cast(tf.math.ceil(sqrt_N), \"int32\")\n",
    "        patch_pos_embed = tf.reshape(\n",
    "            patch_pos_embed, (1, sqrt_N_ceil, sqrt_N_ceil, dim)\n",
    "        )\n",
    "        patch_pos_embed = tf.image.resize(patch_pos_embed, (h0, w0), method=\"bicubic\")\n",
    "\n",
    "        tf.debugging.assert_equal(h0, shape_list(patch_pos_embed)[1])\n",
    "        tf.debugging.assert_equal(w0, shape_list(patch_pos_embed)[2])\n",
    "\n",
    "        patch_pos_embed = tf.reshape(patch_pos_embed, (1, -1, dim))\n",
    "        return tf.concat([class_pos_embed[None, ...], patch_pos_embed], axis=1)\n",
    "\n",
    "    def interpolate_pos_embedding(self, x, h, w):\n",
    "        \"\"\"Resizes the positional embedding in case there is a mismatch in resolution.\n",
    "        E.g., using 480x480 images instead of 224x224 for a given patch size.\n",
    "\n",
    "        Reference:\n",
    "            https://github.com/facebookresearch/dino/blob/main/vision_transformer.py#L174\n",
    "        \"\"\"\n",
    "        num_patches = shape_list(x)[1] - 1  # Exlcuding the cls token.\n",
    "        N = shape_list(self.positional_embedding)[1] - 1\n",
    "\n",
    "        # Segregate the cls embedding from the position embeddings.\n",
    "        class_pos_embed = self.positional_embedding[:, 0]\n",
    "        patch_pos_embed = self.positional_embedding[:, 1:]\n",
    "        dim = shape_list(x)[-1]\n",
    "\n",
    "        pos_embed = tf.cond(\n",
    "            tf.logical_and(tf.equal(num_patches, N), tf.equal(h, w)),\n",
    "            lambda: self.positional_embedding,\n",
    "            lambda: self.interpolate_pos(x, N, h, w),\n",
    "        )\n",
    "        return pos_embed\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        n, h, w, c = shape_list(inputs)\n",
    "\n",
    "        # Create patches and project the patches.\n",
    "        projected_patches = self.projection(inputs)\n",
    "\n",
    "        # Append class token.\n",
    "        cls_token = tf.tile(self.cls_token, (n, 1, 1))\n",
    "        cls_token = tf.cast(cls_token, projected_patches.dtype)\n",
    "        projected_patches = tf.concat([cls_token, projected_patches], axis=1)\n",
    "\n",
    "        # Fetch positional embeddings.\n",
    "        positional_embedding = self.interpolate_pos_embedding(projected_patches, h, w)\n",
    "\n",
    "        # Add positional embeddings to the projected patches.\n",
    "        encoded_patches = (\n",
    "            positional_embedding + projected_patches\n",
    "        )  # (B, number_patches, projection_dim)\n",
    "        encoded_patches = self.dropout(encoded_patches)\n",
    "\n",
    "        # Initialize a dictionary to store attention scores from each transformer\n",
    "        # block.\n",
    "        attention_scores = dict()\n",
    "\n",
    "        # Iterate over the number of layers and stack up blocks of\n",
    "        # Transformer.\n",
    "        for transformer_module in self.transformer_blocks:\n",
    "            # Add a Transformer block.\n",
    "            encoded_patches, attention_score = transformer_module(encoded_patches)\n",
    "            attention_scores[f\"{transformer_module.name}_att\"] = attention_score\n",
    "\n",
    "        # Final layer normalization.\n",
    "        representation = self.layer_norm(encoded_patches)\n",
    "\n",
    "        # Pool representation.\n",
    "        encoded_patches = representation[:, 0]\n",
    "\n",
    "        # Classification head.\n",
    "        output = self.head(encoded_patches)\n",
    "\n",
    "        if training:\n",
    "            return output\n",
    "        else:\n",
    "            return output, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZMn5v3G3uxF",
    "outputId": "2a2ec419-ee68-4ef7-b158-3e7c3e837914"
   },
   "outputs": [],
   "source": [
    "config = base_config.get_config(model_name=\"vit_base\", projection_dim=768, num_heads=12)\n",
    "\n",
    "vit_b16_in1k = ViTB16Extended(config)\n",
    "\n",
    "dummy_inputs = tf.random.normal((2, 224, 224, 3))\n",
    "outputs, attn_scores = vit_b16_in1k(dummy_inputs)\n",
    "\n",
    "keys = list(attn_scores.keys())\n",
    "print(attn_scores[keys[-1]].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the pre-trained params into the TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qh-1yVWY4BTw",
    "outputId": "69a2e272-63c4-4bc7-d1dc-34d1a6444bce"
   },
   "outputs": [],
   "source": [
    "# Projection.\n",
    "\n",
    "vit_b16_in1k.layers[0].layers[0].kernel.assign(\n",
    "    tf.Variable(params_jax[\"embedding/kernel\"])\n",
    ")\n",
    "vit_b16_in1k.layers[0].layers[0].bias.assign(tf.Variable(params_jax[\"embedding/bias\"]))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcJEWo8r4cEc",
    "outputId": "1b4ecfc9-f4fe-4c0a-b251-757a02a48f18"
   },
   "outputs": [],
   "source": [
    "# Positional embedding.\n",
    "\n",
    "vit_b16_in1k.positional_embedding.assign(\n",
    "    tf.Variable(params_jax[\"Transformer/posembed_input/pos_embedding\"])\n",
    ")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sS_CRktp4eq8",
    "outputId": "eede536b-fa50-4ae8-8ba0-b7ce7c795096"
   },
   "outputs": [],
   "source": [
    "# Cls token.\n",
    "\n",
    "vit_b16_in1k.cls_token.assign(tf.Variable(params_jax[\"cls\"]))\n",
    "print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-S1vKGfa4m5r",
    "outputId": "746f3b65-67f3-4d64-d369-2c3963d7556c"
   },
   "outputs": [],
   "source": [
    "vit_b16_in1k.layers[-2].gamma.assign(\n",
    "    tf.Variable(params_jax[\"Transformer/encoder_norm/scale\"])\n",
    ")\n",
    "vit_b16_in1k.layers[-2].beta.assign(\n",
    "    tf.Variable(params_jax[\"Transformer/encoder_norm/bias\"])\n",
    ")\n",
    "\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLknjcqu40uz",
    "outputId": "1d3229f0-c241-46c3-f495-3a210ca610dd"
   },
   "outputs": [],
   "source": [
    "vit_b16_in1k.layers[-1].kernel.assign(tf.Variable(params_jax[\"head/kernel\"]))\n",
    "vit_b16_in1k.layers[-1].bias.assign(tf.Variable(params_jax[\"head/bias\"]))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXDWfket44pg"
   },
   "outputs": [],
   "source": [
    "def modify_attention_block(tf_component, jax_component, params_jax, config):\n",
    "    tf_component.kernel.assign(\n",
    "        tf.Variable(\n",
    "            params_jax[f\"{jax_component}/kernel\"].reshape(config.projection_dim, -1)\n",
    "        )\n",
    "    )\n",
    "    tf_component.bias.assign(\n",
    "        tf.Variable(\n",
    "            params_jax[f\"{jax_component}/bias\"].reshape(-1)\n",
    "        )\n",
    "    )\n",
    "    return tf_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBplXZdp48ha"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for outer_layer in vit_b16_in1k.layers:\n",
    "    if isinstance(outer_layer, tf.keras.Model) and outer_layer.name != \"projection\":\n",
    "        tf_block = vit_b16_in1k.get_layer(outer_layer.name)\n",
    "        jax_block_name = f\"encoderblock_{idx}\"\n",
    "\n",
    "        # LayerNorm layers.\n",
    "        layer_norm_idx = 0\n",
    "        for layer in tf_block.layers:\n",
    "            if isinstance(layer, tf.keras.layers.LayerNormalization):\n",
    "                layer_norm_jax_prefix = (\n",
    "                    f\"Transformer/{jax_block_name}/LayerNorm_{layer_norm_idx}\"\n",
    "                )\n",
    "                layer.gamma.assign(\n",
    "                    tf.Variable(params_jax[f\"{layer_norm_jax_prefix}/scale\"])\n",
    "                )\n",
    "                layer.beta.assign(\n",
    "                    tf.Variable(params_jax[f\"{layer_norm_jax_prefix}/bias\"])\n",
    "                )\n",
    "                layer_norm_idx += 2\n",
    "\n",
    "        # FFN layers.\n",
    "        ffn_layer_idx = 0\n",
    "        for layer in tf_block.layers:\n",
    "            if isinstance(layer, tf.keras.layers.Dense):\n",
    "                dense_layer_jax_prefix = (\n",
    "                    f\"Transformer/{jax_block_name}/MlpBlock_3/Dense_{ffn_layer_idx}\"\n",
    "                )\n",
    "                layer.kernel.assign(\n",
    "                    tf.Variable(params_jax[f\"{dense_layer_jax_prefix}/kernel\"])\n",
    "                )\n",
    "                layer.bias.assign(\n",
    "                    tf.Variable(params_jax[f\"{dense_layer_jax_prefix}/bias\"])\n",
    "                )\n",
    "                ffn_layer_idx += 1\n",
    "\n",
    "        # Attention layer.\n",
    "        for layer in tf_block.layers:\n",
    "            attn_layer_jax_prefix = (\n",
    "                f\"Transformer/{jax_block_name}/MultiHeadDotProductAttention_1\"\n",
    "            )\n",
    "            if isinstance(layer, mha.TFViTAttention):\n",
    "                # Key\n",
    "                layer.self_attention.key = modify_attention_block(\n",
    "                    layer.self_attention.key,\n",
    "                    f\"{attn_layer_jax_prefix}/key\",\n",
    "                    params_jax,\n",
    "                    config,\n",
    "                )\n",
    "                # Query\n",
    "                layer.self_attention.query = modify_attention_block(\n",
    "                    layer.self_attention.query,\n",
    "                    f\"{attn_layer_jax_prefix}/query\",\n",
    "                    params_jax,\n",
    "                    config,\n",
    "                )\n",
    "                # Value\n",
    "                layer.self_attention.value = modify_attention_block(\n",
    "                    layer.self_attention.value,\n",
    "                    f\"{attn_layer_jax_prefix}/value\",\n",
    "                    params_jax,\n",
    "                    config,\n",
    "                )\n",
    "                # Final dense projection\n",
    "                layer.dense_output.dense.kernel.assign(\n",
    "                    tf.Variable(\n",
    "                        params_jax[f\"{attn_layer_jax_prefix}/out/kernel\"].reshape(\n",
    "                            -1, config.projection_dim\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                layer.dense_output.dense.bias.assign(\n",
    "                    tf.Variable(params_jax[f\"{attn_layer_jax_prefix}/out/bias\"])\n",
    "                )\n",
    "\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the populated model on a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeeatlSD5Jc7"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khpDsJFg5L04",
    "outputId": "720a4988-1d33-445e-d31b-5dca3578554b"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = np.array(image)\n",
    "    image_resized = tf.image.resize(image, (224, 224))\n",
    "    image_resized = tf.cast(image_resized, tf.float32)\n",
    "    image_resized = (image_resized - 127.5) / 127.5\n",
    "    return tf.expand_dims(image_resized, 0).numpy()\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    image = preprocess_image(image)\n",
    "    return image\n",
    "\n",
    "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt -O ilsvrc2012_wordnet_lemmas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQMtafdh5N25"
   },
   "outputs": [],
   "source": [
    "with open(\"ilsvrc2012_wordnet_lemmas.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "imagenet_int_to_str = [line.rstrip() for line in lines]\n",
    "\n",
    "img_url = \"https://p0.pikrepo.com/preview/853/907/close-up-photo-of-gray-elephant.jpg\"\n",
    "image = load_image_from_url(img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJS0ddoG5Px6"
   },
   "outputs": [],
   "source": [
    "predictions = vit_b16_in1k(image, training=False)\n",
    "logits = predictions[0].numpy()\n",
    "predicted_label = imagenet_int_to_str[int(np.argmax(logits))]\n",
    "expected_label = \"Indian_elephant, Elephas_maximus\"\n",
    "assert (\n",
    "    predicted_label == expected_label\n",
    "), f\"Expected {expected_label} but was {predicted_label}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video generation utilities\n",
    "\n",
    "Code for the `VideoGeneratorTF` class has been copied and modified from [here](https://github.com/facebookresearch/dino/blob/main/video_generation.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPXBG1x173m6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj6gDwup8Ag0"
   },
   "outputs": [],
   "source": [
    "FOURCC = {\n",
    "    \"mp4\": cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
    "    \"avi\": cv2.VideoWriter_fourcc(*\"XVID\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LU9GCv28CP7"
   },
   "outputs": [],
   "source": [
    "class VideoGeneratorTF:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def run(self):\n",
    "        if self.args.input_path is None:\n",
    "            print(f\"Provided input path {self.args.input_path} is non valid.\")\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            if self.args.video_only:\n",
    "                self._generate_video_from_images(\n",
    "                    self.args.input_path, self.args.output_path\n",
    "                )\n",
    "            else:\n",
    "                # If input path exists\n",
    "                if os.path.exists(self.args.input_path):\n",
    "                    # If input is a video file\n",
    "                    if os.path.isfile(self.args.input_path):\n",
    "                        frames_folder = os.path.join(self.args.output_path, \"frames-tf\")\n",
    "                        attention_folder = os.path.join(\n",
    "                            self.args.output_path, \"attention-tf\"\n",
    "                        )\n",
    "\n",
    "                        os.makedirs(frames_folder, exist_ok=True)\n",
    "                        os.makedirs(attention_folder, exist_ok=True)\n",
    "\n",
    "                        self._extract_frames_from_video(\n",
    "                            self.args.input_path, frames_folder\n",
    "                        )\n",
    "\n",
    "                        self._inference(\n",
    "                            frames_folder,\n",
    "                            attention_folder,\n",
    "                        )\n",
    "\n",
    "                        self._generate_video_from_images(\n",
    "                            attention_folder, self.args.output_path\n",
    "                        )\n",
    "\n",
    "                    # If input is a folder of already extracted frames\n",
    "                    if os.path.isdir(self.args.input_path):\n",
    "                        attention_folder = os.path.join(\n",
    "                            self.args.output_path, \"attention-tf\"\n",
    "                        )\n",
    "\n",
    "                        os.makedirs(attention_folder, exist_ok=True)\n",
    "\n",
    "                        self._inference(self.args.input_path, attention_folder)\n",
    "\n",
    "                        self._generate_video_from_images(\n",
    "                            attention_folder, self.args.output_path\n",
    "                        )\n",
    "\n",
    "                # If input path doesn't exists\n",
    "                else:\n",
    "                    print(f\"Provided input path {self.args.input_path} doesn't exists.\")\n",
    "                    sys.exit(1)\n",
    "\n",
    "    def _extract_frames_from_video(self, inp: str, out: str):\n",
    "        vidcap = cv2.VideoCapture(inp)\n",
    "        self.args.fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        print(f\"Video: {inp} ({self.args.fps} fps)\")\n",
    "        print(f\"Extracting frames to {out}\")\n",
    "\n",
    "        success, image = vidcap.read()\n",
    "        count = 0\n",
    "        while success:\n",
    "            cv2.imwrite(\n",
    "                os.path.join(out, f\"frame-{count:04}.jpg\"),\n",
    "                image,\n",
    "            )\n",
    "            success, image = vidcap.read()\n",
    "            count += 1\n",
    "\n",
    "    def _generate_video_from_images(self, inp: str, out: str):\n",
    "        img_array = []\n",
    "        attention_images_list = sorted(glob.glob(os.path.join(inp, \"attn-*.jpg\")))\n",
    "\n",
    "        # Get size of the first image\n",
    "        with open(attention_images_list[0], \"rb\") as f:\n",
    "            img = Image.open(f)\n",
    "            img = img.convert(\"RGB\")\n",
    "            size = (img.width, img.height)\n",
    "            img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        print(f\"Generating video {size} to {out}\")\n",
    "\n",
    "        for filename in tqdm(attention_images_list[1:]):\n",
    "            with open(filename, \"rb\") as f:\n",
    "                img = Image.open(f)\n",
    "                img = img.convert(\"RGB\")\n",
    "                img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        out = cv2.VideoWriter(\n",
    "            os.path.join(out, \"video-tf.\" + self.args.video_format),\n",
    "            FOURCC[self.args.video_format],\n",
    "            self.args.fps,\n",
    "            size,\n",
    "        )\n",
    "\n",
    "        for i in range(len(img_array)):\n",
    "            out.write(img_array[i])\n",
    "        out.release()\n",
    "        print(\"Done\")\n",
    "\n",
    "    def _preprocess_image(self, image: Image, size: int):\n",
    "        # Reference: https://www.tensorflow.org/lite/examples/style_transfer/overview\n",
    "        image = np.array(image)\n",
    "        image_resized = tf.expand_dims(image, 0)\n",
    "        shape = tf.cast(tf.shape(image_resized)[1:-1], tf.float32)\n",
    "        short_dim = min(shape)\n",
    "        scale = size / short_dim\n",
    "        new_shape = tf.cast(shape * scale, tf.int32)\n",
    "        image_resized = tf.image.resize(\n",
    "            image_resized,\n",
    "            new_shape,\n",
    "        )\n",
    "        image_resized = (image_resized - 127.5) / 127.5\n",
    "        return (image_resized).numpy()\n",
    "\n",
    "    def _inference(self, inp: str, out: str):\n",
    "        print(f\"Generating attention images to {out}\")\n",
    "\n",
    "        for img_path in tqdm(sorted(glob.glob(os.path.join(inp, \"*.jpg\")))):\n",
    "            with open(img_path, \"rb\") as f:\n",
    "                img = Image.open(f)\n",
    "                img = img.convert(\"RGB\")\n",
    "\n",
    "            preprocessed_image = self._preprocess_image(img, self.args.resize)\n",
    "            h, w = (\n",
    "                preprocessed_image.shape[1]\n",
    "                - preprocessed_image.shape[1] % self.args.patch_size,\n",
    "                preprocessed_image.shape[2]\n",
    "                - preprocessed_image.shape[2] % self.args.patch_size,\n",
    "            )\n",
    "            preprocessed_image = preprocessed_image[:, :h, :w, :]\n",
    "\n",
    "            h_featmap = preprocessed_image.shape[1] // self.args.patch_size\n",
    "            w_featmap = preprocessed_image.shape[2] // self.args.patch_size\n",
    "\n",
    "            # Grab the attention scores from the final transformer block.\n",
    "            logits, attention_score_dict = self.args.model(\n",
    "                preprocessed_image, training=False\n",
    "            )\n",
    "            attentions = attention_score_dict[\"transformer_block_11_att\"].numpy()\n",
    "\n",
    "            nh = attentions.shape[1]  # number of head\n",
    "\n",
    "            # we keep only the output patch attention\n",
    "            attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "            attentions = attentions.reshape(nh, h_featmap, w_featmap)\n",
    "            attentions = attentions.transpose((1, 2, 0))\n",
    "\n",
    "            # interpolate\n",
    "            attentions = tf.image.resize(\n",
    "                attentions,\n",
    "                size=(\n",
    "                    h_featmap * self.args.patch_size,\n",
    "                    w_featmap * self.args.patch_size,\n",
    "                ),\n",
    "            )\n",
    "            # attentions = tf.keras.layers.UpSampling2D(size=self.args.patch_size, data_format=\"channels_first\")(\n",
    "            #     attentions[None, ...]\n",
    "            # )[0].numpy()\n",
    "\n",
    "            # save attentions heatmaps\n",
    "            fname = os.path.join(out, \"attn-\" + os.path.basename(img_path))\n",
    "            plt.imsave(\n",
    "                fname=fname,\n",
    "                arr=sum(\n",
    "                    attentions[..., i] * 1 / attentions.shape[-1]\n",
    "                    for i in range(attentions.shape[-1])\n",
    "                ),\n",
    "                cmap=\"inferno\",\n",
    "                format=\"jpg\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather demo videos to run inference on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNYEo7fd8ex7",
    "outputId": "53b9cf8a-5a14-409e-de2f-99e621f999b8"
   },
   "outputs": [],
   "source": [
    "# Get a demo video.\n",
    "!gdown --id 12KScLSdZS5gNvLqoZBenbYeTPaVx4wMj\n",
    "!gdown --id 1dnPP0QvJ2944GaSE47yMgrt3T0yO4R_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CHOBjb38iEa"
   },
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "\n",
    "args = ml_collections.ConfigDict()\n",
    "\n",
    "args.model = vit_b16_in1k\n",
    "args.patch_size = 16\n",
    "args.input_path = \"dog.mp4\"\n",
    "args.output_path = \"./\"\n",
    "args.resize = 512\n",
    "args.video_only = False\n",
    "args.fps = 30.0\n",
    "args.video_format = \"mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract frames, run inference, prepare a video assembling the extracted attention heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIRvBAKN8oBS",
    "outputId": "1588cdc2-d1c5-4c14-8485-f2862a83e0ac"
   },
   "outputs": [],
   "source": [
    "vg = VideoGeneratorTF(args)\n",
    "vg.run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vitb16-attention-maps-video.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
