{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d80858",
   "metadata": {},
   "source": [
    "This notebook visualizes the attention scores from individual attention heads (from the final Transformer block) as [DINO does](https://arxiv.org/abs/2104.14294). \n",
    "\n",
    "Code used in this notebook has been copied and modified from the [official DINO implementation](https://github.com/facebookresearch/dino/blob/main/visualize_attention.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa337fe",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc22501",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U gdown -q\n",
    "!gdown --id 16_1oDm0PeCGJ_KGBG5UKVN7TsAtiRNrN\n",
    "!unzip -q vit_dino_base16.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e826d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d0246",
   "metadata": {},
   "source": [
    "## Data utilities\n",
    "\n",
    "**Notes**\n",
    "\n",
    "This notebook can be extended to visualize the attention maps of all the models that we cover (ViT, DeiT, and DINO).\n",
    "If you're using [DeiT models](https://tfhub.dev/sayakpaul/collections/deit/1) or the DINO model then you should use the following preprocessing utilities.\n",
    "\n",
    "If you're using the original ViT models (mentioned in the main README of this repository) then you should scale the image pixels to [-1, 1] range instead and _not normalize them like the following_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_resolution = 224\n",
    "patch_size = 16\n",
    "\n",
    "crop_layer = keras.layers.CenterCrop(input_resolution, input_resolution)\n",
    "norm_layer = keras.layers.Normalization(\n",
    "    mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "    variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
    ") \n",
    "\n",
    "\n",
    "def preprocess_image(image, size=input_resolution):\n",
    "    image = np.array(image)\n",
    "    image_resized = tf.expand_dims(image, 0)\n",
    "    resize_size = int((256 / 224) * size)\n",
    "    image_resized = tf.image.resize(\n",
    "        image_resized, (resize_size, resize_size), method=\"bicubic\"\n",
    "    )\n",
    "    image_resized = crop_layer(image_resized)\n",
    "    return norm_layer(image_resized).numpy()\n",
    "\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    # Credit: Willi Gierke\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    return image, preprocessed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, preprocessed_image = load_image_from_url(\n",
    "    \"https://dl.fbaipublicfiles.com/dino/img.png\"\n",
    ")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c4d08",
   "metadata": {},
   "source": [
    "## Load the TF DINO model and access the attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dino = tf.keras.models.load_model(\"vit_dino_base16\")\n",
    "\n",
    "logits, attention_score_dict = tf_dino.predict(preprocessed_image)\n",
    "print(attention_score_dict.keys())\n",
    "\n",
    "# Last transformer block.\n",
    "attention_scores = attention_score_dict[\"transformer_block_11_att\"]\n",
    "print(attention_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0609db5",
   "metadata": {},
   "source": [
    "## Preprocess the attention scores and visualize them\n",
    "\n",
    "Currently this notebook can be used for all the models that we cover in the repository (ViT, DeiT, and DINO) with the image resolution `224x224`. If you want to visualize the attention maps for any generic image resolution please follow `dino-attention-maps-video.ipynb` and adapt the code. We do not incorporate the encoding interpolation in this notebook in the interest of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ead13",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_featmap = preprocessed_image.shape[2] // 16\n",
    "h_featmap = preprocessed_image.shape[1] // 16\n",
    "\n",
    "nh = attention_scores.shape[1]  # Number of attention heads.\n",
    "\n",
    "# Taking the representations from CLS token.\n",
    "attentions = attention_scores[0, :, 0, 1:].reshape(nh, -1)\n",
    "print(attentions.shape)\n",
    "\n",
    "# Reshape the attention scores to resemble mini patches.\n",
    "attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "attentions = attentions.transpose((1, 2, 0))\n",
    "print(attentions.shape)\n",
    "\n",
    "# Resize the attention patches to 224x224 (224: 14x16)\n",
    "attentions = tf.image.resize(attentions, size=(h_featmap*16, w_featmap*16))\n",
    "print(attentions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(13, 13))\n",
    "img_count = 0\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        if img_count < len(attentions):\n",
    "            axes[i, j].imshow(attentions[img_count])\n",
    "            axes[i, j].title.set_text(f\"Attention head: {img_count}\")\n",
    "            axes[i, j].axis(\"off\")\n",
    "            img_count += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"dino_attention_heads\", dpi=300, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
