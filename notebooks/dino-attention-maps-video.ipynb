{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook visualizes the attention scores from individual attention heads (from the final Transformer block) as [DINO does](https://arxiv.org/abs/2104.14294). \n",
    "\n",
    "Code used in this notebook has been copied and modified from the [official DINO implementation](https://github.com/facebookresearch/dino/blob/main/video_generation.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "411fTS64Pth7",
    "outputId": "3a0ef495-61ca-4058-8747-e8a1d65667ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |████▏                           | 10 kB 16.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 20 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 30 kB 7.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 40 kB 6.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 51 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 61 kB 4.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 71 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 77 kB 2.7 MB/s \n",
      "\u001b[?25h  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q ml_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIYdn1woOS1n",
    "outputId": "2b21afab-7593-4f40-c6d3-130d81881932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  category=FutureWarning,\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=12KScLSdZS5gNvLqoZBenbYeTPaVx4wMj\n",
      "To: /content/dog.mp4\n",
      "100% 12.8M/12.8M [00:00<00:00, 38.4MB/s]\n",
      "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  category=FutureWarning,\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=16_1oDm0PeCGJ_KGBG5UKVN7TsAtiRNrN\n",
      "To: /content/vit_dino_base16.zip\n",
      "100% 326M/326M [00:02<00:00, 149MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q gdown\n",
    "!gdown --id 12KScLSdZS5gNvLqoZBenbYeTPaVx4wMj\n",
    "!gdown --id 16_1oDm0PeCGJ_KGBG5UKVN7TsAtiRNrN\n",
    "!unzip -q vit_dino_base16.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5SBiIwZ0-PQt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_NJHMizh-Wsd"
   },
   "outputs": [],
   "source": [
    "FOURCC = {\n",
    "    \"mp4\": cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
    "    \"avi\": cv2.VideoWriter_fourcc(*\"XVID\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video generator class inspired from DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-OWPQrlICWBi"
   },
   "outputs": [],
   "source": [
    "class VideoGeneratorTF:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        if self.args.resize != 224:\n",
    "            raise ValueError(\n",
    "                \"We currently support resizing to only 224x224 resolution :(\"\n",
    "            )\n",
    "\n",
    "        if not self.args.video_only:\n",
    "            self.model = self.__load_model()\n",
    "\n",
    "        # For DeiT, DINO this should be unchanged. For the original ViT-B16 models,\n",
    "        # input images should be scaled to [-1, 1] range.\n",
    "        self.norm_layer = keras.layers.Normalization(\n",
    "            mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "            variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        if self.args.input_path is None:\n",
    "            print(f\"Provided input path {self.args.input_path} is non valid.\")\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            if self.args.video_only:\n",
    "                self._generate_video_from_images(\n",
    "                    self.args.input_path, self.args.output_path\n",
    "                )\n",
    "            else:\n",
    "                # If input path exists\n",
    "                if os.path.exists(self.args.input_path):\n",
    "                    # If input is a video file\n",
    "                    if os.path.isfile(self.args.input_path):\n",
    "                        frames_folder = os.path.join(self.args.output_path, \"frames-tf\")\n",
    "                        attention_folder = os.path.join(\n",
    "                            self.args.output_path, \"attention-tf\"\n",
    "                        )\n",
    "\n",
    "                        os.makedirs(frames_folder, exist_ok=True)\n",
    "                        os.makedirs(attention_folder, exist_ok=True)\n",
    "\n",
    "                        self._extract_frames_from_video(\n",
    "                            self.args.input_path, frames_folder\n",
    "                        )\n",
    "\n",
    "                        self._inference(\n",
    "                            frames_folder,\n",
    "                            attention_folder,\n",
    "                        )\n",
    "\n",
    "                        self._generate_video_from_images(\n",
    "                            attention_folder, self.args.output_path\n",
    "                        )\n",
    "\n",
    "                    # If input is a folder of already extracted frames\n",
    "                    if os.path.isdir(self.args.input_path):\n",
    "                        attention_folder = os.path.join(\n",
    "                            self.args.output_path, \"attention-tf\"\n",
    "                        )\n",
    "\n",
    "                        os.makedirs(attention_folder, exist_ok=True)\n",
    "\n",
    "                        self._inference(self.args.input_path, attention_folder)\n",
    "\n",
    "                        self._generate_video_from_images(\n",
    "                            attention_folder, self.args.output_path\n",
    "                        )\n",
    "\n",
    "                # If input path doesn't exists\n",
    "                else:\n",
    "                    print(f\"Provided input path {self.args.input_path} doesn't exists.\")\n",
    "                    sys.exit(1)\n",
    "\n",
    "    def _extract_frames_from_video(self, inp: str, out: str):\n",
    "        vidcap = cv2.VideoCapture(inp)\n",
    "        self.args.fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        print(f\"Video: {inp} ({self.args.fps} fps)\")\n",
    "        print(f\"Extracting frames to {out}\")\n",
    "\n",
    "        success, image = vidcap.read()\n",
    "        count = 0\n",
    "        while success:\n",
    "            cv2.imwrite(\n",
    "                os.path.join(out, f\"frame-{count:04}.jpg\"),\n",
    "                image,\n",
    "            )\n",
    "            success, image = vidcap.read()\n",
    "            count += 1\n",
    "\n",
    "    def _generate_video_from_images(self, inp: str, out: str):\n",
    "        img_array = []\n",
    "        attention_images_list = sorted(glob.glob(os.path.join(inp, \"attn-*.jpg\")))\n",
    "\n",
    "        # Get size of the first image\n",
    "        with open(attention_images_list[0], \"rb\") as f:\n",
    "            img = Image.open(f)\n",
    "            img = img.convert(\"RGB\")\n",
    "            size = (img.width, img.height)\n",
    "            img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        print(f\"Generating video {size} to {out}\")\n",
    "\n",
    "        for filename in tqdm(attention_images_list[1:]):\n",
    "            with open(filename, \"rb\") as f:\n",
    "                img = Image.open(f)\n",
    "                img = img.convert(\"RGB\")\n",
    "                img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        out = cv2.VideoWriter(\n",
    "            os.path.join(out, \"video-tf.\" + self.args.video_format),\n",
    "            FOURCC[self.args.video_format],\n",
    "            self.args.fps,\n",
    "            size,\n",
    "        )\n",
    "\n",
    "        for i in range(len(img_array)):\n",
    "            out.write(img_array[i])\n",
    "        out.release()\n",
    "        print(\"Done\")\n",
    "\n",
    "    def _preprocess_image(self, image: Image, size: int):\n",
    "        image = np.array(image)\n",
    "        image_resized = tf.expand_dims(image, 0)\n",
    "        image_resized = tf.image.resize(image_resized, (size, size))\n",
    "        image_w_ar = tf.image.resize(\n",
    "            image, (size // 2, size // 2), preserve_aspect_ratio=True\n",
    "        )\n",
    "\n",
    "        return self.norm_layer(image_resized).numpy(), image_w_ar.numpy()\n",
    "\n",
    "    def _inference(self, inp: str, out: str):\n",
    "        print(f\"Generating attention images to {out}\")\n",
    "\n",
    "        for img_path in tqdm(sorted(glob.glob(os.path.join(inp, \"*.jpg\")))):\n",
    "            with open(img_path, \"rb\") as f:\n",
    "                img = Image.open(f)\n",
    "                img = img.convert(\"RGB\")\n",
    "\n",
    "            preprocessed_image, image_w_ar = self._preprocess_image(\n",
    "                img, self.args.resize\n",
    "            )\n",
    "\n",
    "            w_featmap = preprocessed_image.shape[2] // self.args.patch_size\n",
    "            h_featmap = preprocessed_image.shape[1] // self.args.patch_size\n",
    "\n",
    "            # Grab the attention scores from the final transformer block.\n",
    "            logits, attention_score_dict = self.model.predict(preprocessed_image)\n",
    "            attentions = attention_score_dict[\"transformer_block_11_att\"]\n",
    "\n",
    "            nh = attentions.shape[1]  # number of head\n",
    "\n",
    "            # we keep only the output patch attention\n",
    "            attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "            attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "    \n",
    "            def _interpolate_attention_map(attn_img):\n",
    "                attn_img = attn_img[..., None]\n",
    "                h, w = image_w_ar.shape[0:-1]\n",
    "                return (\n",
    "                    tf.image.resize(attn_img, (h, w), method=\"nearest\")\n",
    "                    .numpy()\n",
    "                    .squeeze()\n",
    "                )\n",
    "\n",
    "            # save attentions heatmaps\n",
    "            fname = os.path.join(out, \"attn-\" + os.path.basename(img_path))\n",
    "            plt.imsave(\n",
    "                fname=fname,\n",
    "                arr=sum(\n",
    "                    _interpolate_attention_map(attentions[i] * 1 / attentions.shape[0])\n",
    "                    for i in range(attentions.shape[0])\n",
    "                ),\n",
    "                cmap=\"inferno\",\n",
    "                format=\"jpg\",\n",
    "            )\n",
    "\n",
    "    def __load_model(self):\n",
    "        model = keras.models.load_model(self.args.model_path)\n",
    "        print(\"Model loaded.\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5G1jTQMcIrnH"
   },
   "outputs": [],
   "source": [
    "args = ml_collections.ConfigDict()\n",
    "\n",
    "args.model_path = \"vit_dino_base16\"\n",
    "args.patch_size = 16\n",
    "args.pretrained_weights = \"\"\n",
    "args.input_path = \"dog.mp4\"\n",
    "args.output_path = \"./\"\n",
    "args.resize = 224\n",
    "args.video_only = False\n",
    "args.fps = 30.0\n",
    "args.video_format = \"mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7cLtKNZI2rV",
    "outputId": "bab18d51-d526-4c35-9427-0854aff4008e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model loaded.\n",
      "Video: dog.mp4 (29.97002997002997 fps)\n",
      "Extracting frames to ./frames-tf\n",
      "Generating attention images to ./attention-tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating video (112, 63) to ./\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:00<00:00, 2761.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vg = VideoGeneratorTF(args)\n",
    "vg.run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "scratchpad",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "20855a4357ed4c8fb2f8dbe6d07d9791": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b41603a17f4d4d8e97767813141c2ef3",
       "IPY_MODEL_e8334b566e414e12bc07fc74671eee45",
       "IPY_MODEL_795c40698ffd41e0b29bec7354193c78"
      ],
      "layout": "IPY_MODEL_e0748a33e23a41ae8a5872000cdb9f65"
     }
    },
    "40062530255a44cfbfe66a526d84b1ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44f1beb1a05346269e2844c30a115e63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e26d740524944b1a01d49ad813cd06b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "795c40698ffd41e0b29bec7354193c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc87fb5b67634a4ca3e0195db75792e2",
      "placeholder": "​",
      "style": "IPY_MODEL_40062530255a44cfbfe66a526d84b1ef",
      "value": " 327M/327M [00:18&lt;00:00, 27.7MB/s]"
     }
    },
    "b41603a17f4d4d8e97767813141c2ef3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44f1beb1a05346269e2844c30a115e63",
      "placeholder": "​",
      "style": "IPY_MODEL_ea6f14877c634ad1bd7686a73b1004d7",
      "value": "100%"
     }
    },
    "c1130bac199747b2b74699c5243777c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0748a33e23a41ae8a5872000cdb9f65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8334b566e414e12bc07fc74671eee45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1130bac199747b2b74699c5243777c9",
      "max": 343242485,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5e26d740524944b1a01d49ad813cd06b",
      "value": 343242485
     }
    },
    "ea6f14877c634ad1bd7686a73b1004d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc87fb5b67634a4ca3e0195db75792e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
