{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyynYSPNJrh4"
   },
   "source": [
    "## Limitations and disclaimers\n",
    "\n",
    "This notebook shows how to generate attention maps from a video using [DINO ViT-B16 checkpoints](https://github.com/facebookresearch/dino#pretrained-models). We have used existing code snippets from various sources to prepare this demo. We have tried to ensure providing due credits in the respective sections. \n",
    "\n",
    "In order to generate salient attention heatmaps from videos it's important to resize individual frames without losing their aspect ratios. So, it's also important to interpolate the positional embeddings within the ViT model accordingly. Currently, we support this feature through a series of hacks that we aren't very proud of, there's likely a better way to accomplish this in TensorFlow. We're yet to figure that out. \n",
    "\n",
    "Some gotchas you should know about these hacks before proceeding:\n",
    "\n",
    "* With interpolation of positional embeddings, it's currently not possible to save the model. \n",
    "* This is why, we first assemble the original DINO checkpoints for ViT-B16. Then we implement the DINO variant in TensorFlow and port the pre-trained parameters into the implementation manually. \n",
    "* We then run inference and show how to generate attention heatmaps from a given video. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6-zxTdimHLR"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYYHShF8eiws",
    "outputId": "b53dfcee-38fa-413c-8658-585d4778f2ea"
   },
   "outputs": [],
   "source": [
    "!pip install -q ml_collections\n",
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAZf5ObzWZLN"
   },
   "outputs": [],
   "source": [
    "# Backbone\n",
    "!wget https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth -q\n",
    "\n",
    "# Linear layer\n",
    "!wget https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_linearweights.pth -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asb6ucV4mIuB"
   },
   "source": [
    "## Assemble DINO weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIYdn1woOS1n"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUpF7LVAWadr"
   },
   "outputs": [],
   "source": [
    "backbone_state_dict = torch.load(\n",
    "    \"dino_vitbase16_pretrain.pth\", map_location=torch.device(\"cpu\")\n",
    ")\n",
    "linear_layer_state_dict = torch.load(\n",
    "    \"dino_vitbase16_linearweights.pth\", map_location=torch.device(\"cpu\")\n",
    ")[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtMybwZOW5Gr"
   },
   "outputs": [],
   "source": [
    "backbone_state_dict.update(linear_layer_state_dict)\n",
    "backbone_state_dict[\"head.weight\"] = backbone_state_dict.pop(\"module.linear.weight\")\n",
    "backbone_state_dict[\"head.bias\"] = backbone_state_dict.pop(\"module.linear.bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2J3tXVrRmNFs"
   },
   "source": [
    "## Setup model conversion utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYc_zeiRdr0x"
   },
   "outputs": [],
   "source": [
    "!git clone -q https://github.com/sayakpaul/deit-tf -b new-block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXEFXnvbeIed"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"deit-tf\")\n",
    "\n",
    "from vit.vit_models import ViTClassifier\n",
    "from vit.model_configs import base_config\n",
    "from utils import helpers\n",
    "from vit.layers import mha\n",
    "\n",
    "from transformers.tf_utils import shape_list\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import ml_collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgfjk3SRmPpr"
   },
   "source": [
    "## A custom `ViTDINOBase` class to account for DINO's custom representation pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3HNIzlyeREm"
   },
   "outputs": [],
   "source": [
    "class ViTDINOBase(ViTClassifier):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def interpolate_pos(self, x, N, h, w):\n",
    "        class_pos_embed = self.positional_embedding[:, 0]\n",
    "        patch_pos_embed = self.positional_embedding[:, 1:]\n",
    "        dim = shape_list(x)[-1]\n",
    "\n",
    "        # Calculate the resolution to which we need to perform interpolation.\n",
    "        h0 = h // self.config.patch_size\n",
    "        w0 = w // self.config.patch_size\n",
    "\n",
    "        # Reference:\n",
    "        # https://github.com/facebookresearch/dino/blob/main/vision_transformer.py#L186\n",
    "        sqrt_N = tf.math.sqrt(tf.cast(N, \"float32\"))\n",
    "        sqrt_N_ceil = tf.cast(tf.math.ceil(sqrt_N), \"int32\")\n",
    "        patch_pos_embed = tf.reshape(\n",
    "            patch_pos_embed, (1, sqrt_N_ceil, sqrt_N_ceil, dim)\n",
    "        )\n",
    "        patch_pos_embed = tf.image.resize(patch_pos_embed, (h0, w0), method=\"bicubic\")\n",
    "\n",
    "        tf.debugging.assert_equal(h0, shape_list(patch_pos_embed)[1])\n",
    "        tf.debugging.assert_equal(w0, shape_list(patch_pos_embed)[2])\n",
    "\n",
    "        patch_pos_embed = tf.reshape(patch_pos_embed, (1, -1, dim))\n",
    "        return tf.concat([class_pos_embed[None, ...], patch_pos_embed], axis=1)\n",
    "\n",
    "    def interpolate_pos_embedding(self, x, h, w):\n",
    "        \"\"\"Resizes the positional embedding in case there is a mismatch in resolution.\n",
    "        E.g., using 480x480 images instead of 224x224 for a given patch size.\n",
    "\n",
    "        Reference:\n",
    "            https://github.com/facebookresearch/dino/blob/main/vision_transformer.py#L174\n",
    "        \"\"\"\n",
    "        num_patches = shape_list(x)[1] - 1  # Exlcuding the cls token.\n",
    "        N = shape_list(self.positional_embedding)[1] - 1\n",
    "\n",
    "        # Segregate the cls embedding from the position embeddings.\n",
    "        class_pos_embed = self.positional_embedding[:, 0]\n",
    "        patch_pos_embed = self.positional_embedding[:, 1:]\n",
    "        dim = shape_list(x)[-1]\n",
    "\n",
    "        pos_embed = tf.cond(\n",
    "            tf.logical_and(tf.equal(num_patches, N), tf.equal(h, w)),\n",
    "            lambda: self.positional_embedding,\n",
    "            lambda: self.interpolate_pos(x, N, h, w),\n",
    "        )\n",
    "        return pos_embed\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        n, h, w, c = shape_list(inputs)\n",
    "\n",
    "        # Create patches and project the patches.\n",
    "        projected_patches = self.projection(inputs)\n",
    "\n",
    "        # Append class token.\n",
    "        cls_token = tf.tile(self.cls_token, (n, 1, 1))\n",
    "        if cls_token.dtype != projected_patches.dtype:\n",
    "            cls_token = tf.cast(cls_token, projected_patches.dtype)\n",
    "        projected_patches = tf.concat([cls_token, projected_patches], axis=1)\n",
    "\n",
    "        # Fetch positional embeddings.\n",
    "        positional_embedding = self.interpolate_pos_embedding(projected_patches, h, w)\n",
    "\n",
    "        # Add positional embeddings to the projected patches.\n",
    "        encoded_patches = (\n",
    "            positional_embedding + projected_patches\n",
    "        )  # (B, number_patches, projection_dim)\n",
    "        encoded_patches = self.dropout(encoded_patches)\n",
    "\n",
    "        # Initialize a dictionary to store attention scores from each transformer\n",
    "        # block.\n",
    "        attention_scores = dict()\n",
    "\n",
    "        # Iterate over the number of layers and stack up blocks of\n",
    "        # Transformer.\n",
    "        for transformer_module in self.transformer_blocks:\n",
    "            # Add a Transformer block.\n",
    "            encoded_patches, attention_score = transformer_module(encoded_patches)\n",
    "            attention_scores[f\"{transformer_module.name}_att\"] = attention_score\n",
    "\n",
    "        # Final layer normalization.\n",
    "        representation = self.layer_norm(encoded_patches)\n",
    "\n",
    "        # Pool representation.\n",
    "        # Reference: https://github.com/facebookresearch/dino/blob/main/eval_linear.py#L259-#L260\n",
    "        encoded_patches = representation[:, 0]\n",
    "        encoded_patches_exp = tf.expand_dims(encoded_patches, -1)\n",
    "        avg_patch_tokens = tf.reduce_mean(representation[:, 1:], 1)\n",
    "        avg_patch_tokens = tf.expand_dims(avg_patch_tokens, -1)\n",
    "        output = tf.concat([encoded_patches_exp, avg_patch_tokens], -1)\n",
    "        output = tf.reshape(output, (n, -1))\n",
    "\n",
    "        # Classification head.\n",
    "        output = self.head(output)\n",
    "\n",
    "        if training:\n",
    "            return output\n",
    "        else:\n",
    "            return output, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvbwLEshmXZ8"
   },
   "source": [
    "## Validating the initial architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1F3jn02ego4",
    "outputId": "90ad85a0-fa66-4200-9d67-9a96462273b8"
   },
   "outputs": [],
   "source": [
    "config = base_config.get_config(model_name=\"vit_base\", projection_dim=768, num_heads=12)\n",
    "\n",
    "vit_dino_base = ViTDINOBase(config)\n",
    "\n",
    "dummy_inputs = tf.random.normal((2, 224, 224, 3))\n",
    "outputs, attn_scores = vit_dino_base(dummy_inputs)\n",
    "\n",
    "keys = list(attn_scores.keys())\n",
    "print(attn_scores[keys[-1]].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oREXDqYDme9T"
   },
   "source": [
    "## Port pre-trained DINO params\n",
    "\n",
    "Reference: https://github.com/sayakpaul/deit-tf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXk3tOlci9TH"
   },
   "outputs": [],
   "source": [
    "pt_model_dict = {k: backbone_state_dict[k].numpy() for k in backbone_state_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFIi9GeRjA9n"
   },
   "outputs": [],
   "source": [
    "vit_dino_base.layers[0].layers[0] = helpers.modify_tf_block(\n",
    "    vit_dino_base.layers[0].layers[0],\n",
    "    pt_model_dict[\"patch_embed.proj.weight\"],\n",
    "    pt_model_dict[\"patch_embed.proj.bias\"],\n",
    ")\n",
    "\n",
    "# Positional embedding.\n",
    "vit_dino_base.positional_embedding.assign(tf.Variable(pt_model_dict[\"pos_embed\"]))\n",
    "\n",
    "# CLS and (optional) Distillation tokens.\n",
    "# Distillation token won't be present in the models trained without distillation.\n",
    "vit_dino_base.cls_token.assign(tf.Variable(pt_model_dict[\"cls_token\"]))\n",
    "\n",
    "# Layer norm layers.\n",
    "ln_idx = -2\n",
    "vit_dino_base.layers[ln_idx] = helpers.modify_tf_block(\n",
    "    vit_dino_base.layers[ln_idx],\n",
    "    pt_model_dict[\"norm.weight\"],\n",
    "    pt_model_dict[\"norm.bias\"],\n",
    ")\n",
    "\n",
    "# Head layers.\n",
    "head_layer = vit_dino_base.get_layer(\"classification_head\")\n",
    "vit_dino_base.layers[-1] = helpers.modify_tf_block(\n",
    "    head_layer,\n",
    "    pt_model_dict[\"head.weight\"],\n",
    "    pt_model_dict[\"head.bias\"],\n",
    ")\n",
    "\n",
    "# Transformer blocks.\n",
    "idx = 0\n",
    "\n",
    "for outer_layer in vit_dino_base.layers:\n",
    "    if isinstance(outer_layer, tf.keras.Model) and outer_layer.name != \"projection\":\n",
    "        tf_block = vit_dino_base.get_layer(outer_layer.name)\n",
    "        pt_block_name = f\"blocks.{idx}\"\n",
    "\n",
    "        # LayerNorm layers.\n",
    "        layer_norm_idx = 1\n",
    "        for layer in tf_block.layers:\n",
    "            if isinstance(layer, tf.keras.layers.LayerNormalization):\n",
    "                layer_norm_pt_prefix = f\"{pt_block_name}.norm{layer_norm_idx}\"\n",
    "                layer.gamma.assign(\n",
    "                    tf.Variable(pt_model_dict[f\"{layer_norm_pt_prefix}.weight\"])\n",
    "                )\n",
    "                layer.beta.assign(\n",
    "                    tf.Variable(pt_model_dict[f\"{layer_norm_pt_prefix}.bias\"])\n",
    "                )\n",
    "                layer_norm_idx += 1\n",
    "\n",
    "        # FFN layers.\n",
    "        ffn_layer_idx = 1\n",
    "        for layer in tf_block.layers:\n",
    "            if isinstance(layer, tf.keras.layers.Dense):\n",
    "                dense_layer_pt_prefix = f\"{pt_block_name}.mlp.fc{ffn_layer_idx}\"\n",
    "                layer = helpers.modify_tf_block(\n",
    "                    layer,\n",
    "                    pt_model_dict[f\"{dense_layer_pt_prefix}.weight\"],\n",
    "                    pt_model_dict[f\"{dense_layer_pt_prefix}.bias\"],\n",
    "                )\n",
    "                ffn_layer_idx += 1\n",
    "\n",
    "        # Attention layer.\n",
    "        for layer in tf_block.layers:\n",
    "            (q_w, k_w, v_w), (q_b, k_b, v_b) = helpers.get_tf_qkv(\n",
    "                f\"{pt_block_name}.attn\",\n",
    "                pt_model_dict,\n",
    "                config,\n",
    "            )\n",
    "\n",
    "            if isinstance(layer, mha.TFViTAttention):\n",
    "                # Key\n",
    "                layer.self_attention.key = helpers.modify_tf_block(\n",
    "                    layer.self_attention.key,\n",
    "                    k_w,\n",
    "                    k_b,\n",
    "                    is_attn=True,\n",
    "                )\n",
    "                # Query\n",
    "                layer.self_attention.query = helpers.modify_tf_block(\n",
    "                    layer.self_attention.query,\n",
    "                    q_w,\n",
    "                    q_b,\n",
    "                    is_attn=True,\n",
    "                )\n",
    "                # Value\n",
    "                layer.self_attention.value = helpers.modify_tf_block(\n",
    "                    layer.self_attention.value,\n",
    "                    v_w,\n",
    "                    v_b,\n",
    "                    is_attn=True,\n",
    "                )\n",
    "                # Final dense projection\n",
    "                layer.dense_output.dense = helpers.modify_tf_block(\n",
    "                    layer.dense_output.dense,\n",
    "                    pt_model_dict[f\"{pt_block_name}.attn.proj.weight\"],\n",
    "                    pt_model_dict[f\"{pt_block_name}.attn.proj.bias\"],\n",
    "                )\n",
    "\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECLhHQaZJ3j8"
   },
   "source": [
    "## Video generation for attention maps\n",
    "\n",
    "Code copied and modified from the [official code](https://github.com/facebookresearch/dino/blob/main/video_generation.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8HINxrJfrY-"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvTo_f6bgIs3"
   },
   "outputs": [],
   "source": [
    "FOURCC = {\n",
    "    \"mp4\": cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
    "    \"avi\": cv2.VideoWriter_fourcc(*\"XVID\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTclj98bezH_"
   },
   "outputs": [],
   "source": [
    "class VideoGeneratorTF:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        # For DeiT, DINO this should be unchanged. For the original ViT-B16 models,\n",
    "        # input images should be scaled to [-1, 1] range.\n",
    "        self.norm_layer = keras.layers.Normalization(\n",
    "            mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "            variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        if self.args.input_path is None:\n",
    "            print(f\"Provided input path {self.args.input_path} is non valid.\")\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            if self.args.video_only:\n",
    "                self._generate_video_from_images(\n",
    "                    self.args.input_path, self.args.output_path\n",
    "                )\n",
    "            else:\n",
    "                # If input path exists\n",
    "                if os.path.exists(self.args.input_path):\n",
    "                    # If input is a video file\n",
    "                    if os.path.isfile(self.args.input_path):\n",
    "                        frames_folder = os.path.join(self.args.output_path, \"frames-tf\")\n",
    "                        attention_folder = os.path.join(\n",
    "                            self.args.output_path, \"attention-tf\"\n",
    "                        )\n",
    "\n",
    "                        os.makedirs(frames_folder, exist_ok=True)\n",
    "                        os.makedirs(attention_folder, exist_ok=True)\n",
    "\n",
    "                        self._extract_frames_from_video(\n",
    "                            self.args.input_path, frames_folder\n",
    "                        )\n",
    "\n",
    "                        self._inference(\n",
    "                            frames_folder,\n",
    "                            attention_folder,\n",
    "                        )\n",
    "\n",
    "                        self._generate_video_from_images(\n",
    "                            attention_folder, self.args.output_path\n",
    "                        )\n",
    "\n",
    "                    # If input is a folder of already extracted frames\n",
    "                    if os.path.isdir(self.args.input_path):\n",
    "                        attention_folder = os.path.join(\n",
    "                            self.args.output_path, \"attention-tf\"\n",
    "                        )\n",
    "\n",
    "                        os.makedirs(attention_folder, exist_ok=True)\n",
    "\n",
    "                        self._inference(self.args.input_path, attention_folder)\n",
    "\n",
    "                        self._generate_video_from_images(\n",
    "                            attention_folder, self.args.output_path\n",
    "                        )\n",
    "\n",
    "                # If input path doesn't exists\n",
    "                else:\n",
    "                    print(f\"Provided input path {self.args.input_path} doesn't exists.\")\n",
    "                    sys.exit(1)\n",
    "\n",
    "    def _extract_frames_from_video(self, inp: str, out: str):\n",
    "        vidcap = cv2.VideoCapture(inp)\n",
    "        self.args.fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        print(f\"Video: {inp} ({self.args.fps} fps)\")\n",
    "        print(f\"Extracting frames to {out}\")\n",
    "\n",
    "        success, image = vidcap.read()\n",
    "        count = 0\n",
    "        while success:\n",
    "            cv2.imwrite(\n",
    "                os.path.join(out, f\"frame-{count:04}.jpg\"),\n",
    "                image,\n",
    "            )\n",
    "            success, image = vidcap.read()\n",
    "            count += 1\n",
    "\n",
    "    def _generate_video_from_images(self, inp: str, out: str):\n",
    "        img_array = []\n",
    "        attention_images_list = sorted(glob.glob(os.path.join(inp, \"attn-*.jpg\")))\n",
    "\n",
    "        # Get size of the first image\n",
    "        with open(attention_images_list[0], \"rb\") as f:\n",
    "            img = Image.open(f)\n",
    "            img = img.convert(\"RGB\")\n",
    "            size = (img.width, img.height)\n",
    "            img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        print(f\"Generating video {size} to {out}\")\n",
    "\n",
    "        for filename in tqdm(attention_images_list[1:]):\n",
    "            with open(filename, \"rb\") as f:\n",
    "                img = Image.open(f)\n",
    "                img = img.convert(\"RGB\")\n",
    "                img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        out = cv2.VideoWriter(\n",
    "            os.path.join(out, \"video-tf.\" + self.args.video_format),\n",
    "            FOURCC[self.args.video_format],\n",
    "            self.args.fps,\n",
    "            size,\n",
    "        )\n",
    "\n",
    "        for i in range(len(img_array)):\n",
    "            out.write(img_array[i])\n",
    "        out.release()\n",
    "        print(\"Done\")\n",
    "\n",
    "    def _preprocess_image(self, image: Image, size: int):\n",
    "        # Reference: https://www.tensorflow.org/lite/examples/style_transfer/overview\n",
    "        image = np.array(image)\n",
    "        image_resized = tf.expand_dims(image, 0)\n",
    "        shape = tf.cast(tf.shape(image_resized)[1:-1], tf.float32)\n",
    "        short_dim = min(shape)\n",
    "        scale = size / short_dim\n",
    "        new_shape = tf.cast(shape * scale, tf.int32)\n",
    "        image_resized = tf.image.resize(\n",
    "            image_resized,\n",
    "            new_shape,\n",
    "        )\n",
    "        return self.norm_layer(image_resized).numpy()\n",
    "\n",
    "    def _inference(self, inp: str, out: str):\n",
    "        print(f\"Generating attention images to {out}\")\n",
    "\n",
    "        for img_path in tqdm(sorted(glob.glob(os.path.join(inp, \"*.jpg\")))):\n",
    "            with open(img_path, \"rb\") as f:\n",
    "                img = Image.open(f)\n",
    "                img = img.convert(\"RGB\")\n",
    "\n",
    "            preprocessed_image = self._preprocess_image(img, self.args.resize)\n",
    "            h, w = (\n",
    "                preprocessed_image.shape[1]\n",
    "                - preprocessed_image.shape[1] % self.args.patch_size,\n",
    "                preprocessed_image.shape[2]\n",
    "                - preprocessed_image.shape[2] % self.args.patch_size,\n",
    "            )\n",
    "            preprocessed_image = preprocessed_image[:, :h, :w, :]\n",
    "\n",
    "            h_featmap = preprocessed_image.shape[1] // self.args.patch_size\n",
    "            w_featmap = preprocessed_image.shape[2] // self.args.patch_size\n",
    "\n",
    "            # Grab the attention scores from the final transformer block.\n",
    "            logits, attention_score_dict = self.args.model(\n",
    "                preprocessed_image, training=False\n",
    "            )\n",
    "            attentions = attention_score_dict[\"transformer_block_11_att\"].numpy()\n",
    "\n",
    "            nh = attentions.shape[1]  # number of head\n",
    "\n",
    "            # we keep only the output patch attention\n",
    "            attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "            attentions = attentions.reshape(nh, h_featmap, w_featmap)\n",
    "            attentions = attentions.transpose((1, 2, 0))\n",
    "\n",
    "            # interpolate\n",
    "            attentions = tf.image.resize(\n",
    "                attentions,\n",
    "                size=(\n",
    "                    h_featmap * self.args.patch_size,\n",
    "                    w_featmap * self.args.patch_size,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # save attentions heatmaps\n",
    "            fname = os.path.join(out, \"attn-\" + os.path.basename(img_path))\n",
    "            plt.imsave(\n",
    "                fname=fname,\n",
    "                arr=sum(\n",
    "                    attentions[..., i] * 1 / attentions.shape[-1]\n",
    "                    for i in range(attentions.shape[-1])\n",
    "                ),\n",
    "                cmap=\"inferno\",\n",
    "                format=\"jpg\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBHeN4ALnfwj",
    "outputId": "f7684433-9d80-4887-cc8e-d45259f54bb5"
   },
   "outputs": [],
   "source": [
    "# Get demo videos.\n",
    "!gdown --id 12KScLSdZS5gNvLqoZBenbYeTPaVx4wMj\n",
    "!gdown --id 1dnPP0QvJ2944GaSE47yMgrt3T0yO4R_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uiL940Smz2D"
   },
   "outputs": [],
   "source": [
    "args = ml_collections.ConfigDict()\n",
    "\n",
    "args.model = vit_dino_base\n",
    "args.patch_size = 16\n",
    "args.input_path = \"dino.mp4\"\n",
    "args.output_path = \"./\"\n",
    "args.resize = 512\n",
    "args.video_only = False\n",
    "args.fps = 30.0\n",
    "args.video_format = \"mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wx-fQQiTnmBV",
    "outputId": "d99e6d2c-e256-4f88-8b67-79e5d6e9d44a"
   },
   "outputs": [],
   "source": [
    "vg = VideoGeneratorTF(args)\n",
    "vg.run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "dino-attention-maps-video.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
