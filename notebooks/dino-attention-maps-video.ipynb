{"cells":[{"cell_type":"markdown","metadata":{"id":"LvBt1oPDRGiF"},"source":["This notebook visualizes the attention scores from individual attention heads (from the final Transformer block) as [DINO does](https://arxiv.org/abs/2104.14294) and extends the utility for videos. \n","\n","Code used in this notebook has been copied and modified from the [official DINO implementation](https://github.com/facebookresearch/dino/blob/main/video_generation.py). "]},{"cell_type":"markdown","metadata":{"id":"kPGWGmcRRGiI"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"411fTS64Pth7","outputId":"608431d6-7bd8-4986-bf9d-dc049cd042a6","executionInfo":{"status":"ok","timestamp":1649322284787,"user_tz":-330,"elapsed":4399,"user":{"displayName":"Sayak Paul","userId":"05988433726211077895"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\r\u001b[K     |████▏                           | 10 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 30 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 40 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 3.1 MB/s \n","\u001b[?25h  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q ml_collections"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lIYdn1woOS1n","outputId":"6cd8a62d-fc1c-458d-8bad-60a8108b5036","executionInfo":{"status":"ok","timestamp":1649322313728,"user_tz":-330,"elapsed":28945,"user":{"displayName":"Sayak Paul","userId":"05988433726211077895"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=12KScLSdZS5gNvLqoZBenbYeTPaVx4wMj\n","To: /content/dog.mp4\n","100% 12.8M/12.8M [00:00<00:00, 141MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=16_1oDm0PeCGJ_KGBG5UKVN7TsAtiRNrN\n","To: /content/vit_dino_base16.zip\n","100% 326M/326M [00:01<00:00, 193MB/s]\n"]}],"source":["!pip install -U -q gdown\n","!gdown --id 12KScLSdZS5gNvLqoZBenbYeTPaVx4wMj\n","!gdown --id 16_1oDm0PeCGJ_KGBG5UKVN7TsAtiRNrN\n","!unzip -q vit_dino_base16.zip"]},{"cell_type":"markdown","metadata":{"id":"WInZqrBVRGiL"},"source":["## Imports"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"5SBiIwZ0-PQt","executionInfo":{"status":"ok","timestamp":1649322341356,"user_tz":-330,"elapsed":334,"user":{"displayName":"Sayak Paul","userId":"05988433726211077895"}}},"outputs":[],"source":["import os\n","import glob\n","import cv2\n","import sys\n","import ml_collections\n","\n","from PIL import Image\n","from tqdm import tqdm\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_NJHMizh-Wsd","executionInfo":{"status":"ok","timestamp":1649322317193,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sayak Paul","userId":"05988433726211077895"}}},"outputs":[],"source":["FOURCC = {\n","    \"mp4\": cv2.VideoWriter_fourcc(*\"MP4V\"),\n","    \"avi\": cv2.VideoWriter_fourcc(*\"XVID\"),\n","}"]},{"cell_type":"markdown","metadata":{"id":"eeY9JFMyRGiM"},"source":["## Video generator class inspired from DINO"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"-OWPQrlICWBi","executionInfo":{"status":"ok","timestamp":1649322643045,"user_tz":-330,"elapsed":497,"user":{"displayName":"Sayak Paul","userId":"05988433726211077895"}}},"outputs":[],"source":["class VideoGeneratorTF:\n","    def __init__(self, args):\n","        self.args = args\n","\n","        if self.args.resize != 224:\n","            raise ValueError(\n","                \"We currently support resizing to only 224x224 resolution :(\"\n","            )\n","\n","        if not self.args.video_only:\n","            self.model = self.__load_model()\n","\n","        # For DeiT, DINO this should be unchanged. For the original ViT-B16 models,\n","        # input images should be scaled to [-1, 1] range.\n","        self.norm_layer = keras.layers.Normalization(\n","            mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n","            variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n","        )\n","\n","    def run(self):\n","        if self.args.input_path is None:\n","            print(f\"Provided input path {self.args.input_path} is non valid.\")\n","            sys.exit(1)\n","        else:\n","            if self.args.video_only:\n","                self._generate_video_from_images(\n","                    self.args.input_path, self.args.output_path\n","                )\n","            else:\n","                # If input path exists\n","                if os.path.exists(self.args.input_path):\n","                    # If input is a video file\n","                    if os.path.isfile(self.args.input_path):\n","                        frames_folder = os.path.join(self.args.output_path, \"frames-tf\")\n","                        attention_folder = os.path.join(\n","                            self.args.output_path, \"attention-tf\"\n","                        )\n","\n","                        os.makedirs(frames_folder, exist_ok=True)\n","                        os.makedirs(attention_folder, exist_ok=True)\n","\n","                        self._extract_frames_from_video(\n","                            self.args.input_path, frames_folder\n","                        )\n","\n","                        self._inference(\n","                            frames_folder,\n","                            attention_folder,\n","                        )\n","\n","                        self._generate_video_from_images(\n","                            attention_folder, self.args.output_path\n","                        )\n","\n","                    # If input is a folder of already extracted frames\n","                    if os.path.isdir(self.args.input_path):\n","                        attention_folder = os.path.join(\n","                            self.args.output_path, \"attention-tf\"\n","                        )\n","\n","                        os.makedirs(attention_folder, exist_ok=True)\n","\n","                        self._inference(self.args.input_path, attention_folder)\n","\n","                        self._generate_video_from_images(\n","                            attention_folder, self.args.output_path\n","                        )\n","\n","                # If input path doesn't exists\n","                else:\n","                    print(f\"Provided input path {self.args.input_path} doesn't exists.\")\n","                    sys.exit(1)\n","\n","    def _extract_frames_from_video(self, inp: str, out: str):\n","        vidcap = cv2.VideoCapture(inp)\n","        self.args.fps = vidcap.get(cv2.CAP_PROP_FPS)\n","\n","        print(f\"Video: {inp} ({self.args.fps} fps)\")\n","        print(f\"Extracting frames to {out}\")\n","\n","        success, image = vidcap.read()\n","        count = 0\n","        while success:\n","            cv2.imwrite(\n","                os.path.join(out, f\"frame-{count:04}.jpg\"),\n","                image,\n","            )\n","            success, image = vidcap.read()\n","            count += 1\n","\n","    def _generate_video_from_images(self, inp: str, out: str):\n","        img_array = []\n","        attention_images_list = sorted(glob.glob(os.path.join(inp, \"attn-*.jpg\")))\n","\n","        # Get size of the first image\n","        with open(attention_images_list[0], \"rb\") as f:\n","            img = Image.open(f)\n","            img = img.convert(\"RGB\")\n","            size = (img.width, img.height)\n","            img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n","\n","        print(f\"Generating video {size} to {out}\")\n","\n","        for filename in tqdm(attention_images_list[1:]):\n","            with open(filename, \"rb\") as f:\n","                img = Image.open(f)\n","                img = img.convert(\"RGB\")\n","                img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n","\n","        out = cv2.VideoWriter(\n","            os.path.join(out, \"video-tf.\" + self.args.video_format),\n","            FOURCC[self.args.video_format],\n","            self.args.fps,\n","            size,\n","        )\n","\n","        for i in range(len(img_array)):\n","            out.write(img_array[i])\n","        out.release()\n","        print(\"Done\")\n","\n","    def _preprocess_image(self, image: Image, size: int):\n","        image = np.array(image)\n","        image_resized = tf.expand_dims(image, 0)\n","        image_resized = tf.image.resize(image_resized, (size, size))\n","        image_w_ar = tf.image.resize(\n","            image, (size, size), preserve_aspect_ratio=True\n","        )\n","\n","        return self.norm_layer(image_resized).numpy(), image_w_ar.numpy()\n","\n","    def _inference(self, inp: str, out: str):\n","        print(f\"Generating attention images to {out}\")\n","\n","        for img_path in tqdm(sorted(glob.glob(os.path.join(inp, \"*.jpg\")))):\n","            with open(img_path, \"rb\") as f:\n","                img = Image.open(f)\n","                img = img.convert(\"RGB\")\n","\n","            preprocessed_image, image_w_ar = self._preprocess_image(\n","                img, self.args.resize\n","            )\n","\n","            w_featmap = preprocessed_image.shape[2] // self.args.patch_size\n","            h_featmap = preprocessed_image.shape[1] // self.args.patch_size\n","\n","            # Grab the attention scores from the final transformer block.\n","            logits, attention_score_dict = self.model.predict(preprocessed_image)\n","            attentions = attention_score_dict[\"transformer_block_11_att\"]\n","\n","            nh = attentions.shape[1]  # number of head\n","\n","            # we keep only the output patch attention\n","            attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n","            attentions = attentions.reshape(nh, w_featmap, h_featmap)\n","    \n","            def _interpolate_attention_map(attn_img):\n","                attn_img = attn_img[..., None]\n","                h, w = image_w_ar.shape[0:-1]\n","                return (\n","                    tf.image.resize(attn_img, (h, w), method=\"nearest\")\n","                    .numpy()\n","                    .squeeze()\n","                )\n","\n","            # save attentions heatmaps\n","            fname = os.path.join(out, \"attn-\" + os.path.basename(img_path))\n","            plt.imsave(\n","                fname=fname,\n","                arr=sum(\n","                    _interpolate_attention_map(attentions[i] * 1 / attentions.shape[0])\n","                    for i in range(attentions.shape[0])\n","                ),\n","                cmap=\"inferno\",\n","                format=\"jpg\",\n","            )\n","\n","    def __load_model(self):\n","        model = keras.models.load_model(self.args.model_path)\n","        print(\"Model loaded.\")\n","        return model"]},{"cell_type":"markdown","metadata":{"id":"VNCUWjpGRGiP"},"source":["## Run inference"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"5G1jTQMcIrnH","executionInfo":{"status":"ok","timestamp":1649322646228,"user_tz":-330,"elapsed":2,"user":{"displayName":"Sayak Paul","userId":"05988433726211077895"}}},"outputs":[],"source":["args = ml_collections.ConfigDict()\n","\n","args.model_path = \"vit_dino_base16\"\n","args.patch_size = 16\n","args.pretrained_weights = \"\"\n","args.input_path = \"dog.mp4\"\n","args.output_path = \"./\"\n","args.resize = 224\n","args.video_only = False\n","args.fps = 30.0\n","args.video_format = \"mp4\""]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7cLtKNZI2rV","outputId":"3da578b4-28ab-4600-83f0-457d49718928","executionInfo":{"status":"ok","timestamp":1649322759243,"user_tz":-330,"elapsed":111077,"user":{"displayName":"Sayak Paul","userId":"05988433726211077895"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n","Model loaded.\n","Video: dog.mp4 (29.97002997002997 fps)\n","Extracting frames to ./frames-tf\n","Generating attention images to ./attention-tf\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 150/150 [00:59<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Generating video (224, 126) to ./\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 149/149 [00:00<00:00, 1369.35it/s]"]},{"output_type":"stream","name":"stdout","text":["Done\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["vg = VideoGeneratorTF(args)\n","vg.run()"]}],"metadata":{"accelerator":"GPU","colab":{"name":"dino-attention-maps-video.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"nbformat":4,"nbformat_minor":0}