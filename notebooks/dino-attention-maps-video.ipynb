{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations and disclaimers\n",
        "\n",
        "TBD"
      ],
      "metadata": {
        "id": "IyynYSPNJrh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "M6-zxTdimHLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ml_collections\n",
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "uYYHShF8eiws",
        "outputId": "b53dfcee-38fa-413c-8658-585d4778f2ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |████▏                           | 10 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 30 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 40 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 51 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 61 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 71 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 4.1 MB/s \n",
            "\u001b[?25h  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 8.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 54.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 60.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 54.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backbone\n",
        "!wget https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth -q\n",
        "\n",
        "# Linear layer\n",
        "!wget https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_linearweights.pth -q"
      ],
      "metadata": {
        "id": "AAZf5ObzWZLN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assemble DINO weights in PyTorch"
      ],
      "metadata": {
        "id": "asb6ucV4mIuB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "backbone_state_dict = torch.load(\n",
        "    \"dino_vitbase16_pretrain.pth\", map_location=torch.device(\"cpu\")\n",
        ")\n",
        "linear_layer_state_dict = torch.load(\n",
        "    \"dino_vitbase16_linearweights.pth\", map_location=torch.device(\"cpu\")\n",
        ")[\"state_dict\"]"
      ],
      "metadata": {
        "id": "bUpF7LVAWadr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list(backbone_state_dict.keys())"
      ],
      "metadata": {
        "id": "zosMfig4WcIr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list(linear_layer_state_dict.keys())"
      ],
      "metadata": {
        "id": "bkwzZDitWt5f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone_state_dict.update(linear_layer_state_dict)\n",
        "# list(backbone_state_dict.keys())"
      ],
      "metadata": {
        "id": "VtMybwZOW5Gr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone_state_dict[\"head.weight\"] = backbone_state_dict.pop(\"module.linear.weight\")\n",
        "backbone_state_dict[\"head.bias\"] = backbone_state_dict.pop(\"module.linear.bias\")"
      ],
      "metadata": {
        "id": "RekDqWdsXGFF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup model conversion utilities"
      ],
      "metadata": {
        "id": "2J3tXVrRmNFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -q https://github.com/sayakpaul/deit-tf -b new-block"
      ],
      "metadata": {
        "id": "aYc_zeiRdr0x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd deit-tf && git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44S6-_xMkoBR",
        "outputId": "44733fca-f710-4e06-f109-7149bc1880e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"deit-tf\")\n",
        "\n",
        "from vit.vit_models import ViTClassifier\n",
        "from vit.model_configs import base_config\n",
        "from utils import helpers\n",
        "from vit.layers import mha\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import ml_collections"
      ],
      "metadata": {
        "id": "mXEFXnvbeIed"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A custom `ViTDINOBase` class to account for DINO's custom representation pooling"
      ],
      "metadata": {
        "id": "Zgfjk3SRmPpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.tf_utils import shape_list"
      ],
      "metadata": {
        "id": "qAkzcn1YRE0f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTDINOBase(ViTClassifier):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def interpolate_pos(self, x, N, h, w):\n",
        "        class_pos_embed = self.positional_embedding[:, 0]\n",
        "        patch_pos_embed = self.positional_embedding[:, 1:]\n",
        "        dim = shape_list(x)[-1]\n",
        "\n",
        "        # Calculate the resolution to which we need to perform interpolation.\n",
        "        h0 = h // self.config.patch_size\n",
        "        w0 = w // self.config.patch_size\n",
        "\n",
        "        # We avoided the following because UpSampling2D won't support float sizes:\n",
        "        # https://github.com/facebookresearch/dino/blob/main/vision_transformer.py#L186\n",
        "        sqrt_N = tf.math.sqrt(tf.cast(N, \"float32\"))\n",
        "        sqrt_N_ceil = tf.cast(tf.math.ceil(sqrt_N), \"int32\")\n",
        "        patch_pos_embed = tf.reshape(\n",
        "            patch_pos_embed, (1, sqrt_N_ceil, sqrt_N_ceil, dim)\n",
        "        )\n",
        "        patch_pos_embed = tf.image.resize(patch_pos_embed, (h0, w0), method=\"bicubic\")\n",
        "\n",
        "        tf.debugging.assert_equal(h0, shape_list(patch_pos_embed)[1])\n",
        "        tf.debugging.assert_equal(w0, shape_list(patch_pos_embed)[2])\n",
        "\n",
        "        patch_pos_embed = tf.reshape(patch_pos_embed, (1, -1, dim))\n",
        "        return tf.concat([class_pos_embed[None, ...], patch_pos_embed], axis=1)\n",
        "\n",
        "    def interpolate_pos_embedding(self, x, h, w):\n",
        "        \"\"\"Resizes the positional embedding in case there is a mismatch in resolution.\n",
        "        E.g., using 480x480 images instead of 224x224 for a given patch size.\n",
        "\n",
        "        Reference:\n",
        "            https://github.com/facebookresearch/dino/blob/main/vision_transformer.py#L174\n",
        "        \"\"\"\n",
        "        num_patches = shape_list(x)[1] - 1  # Exlcuding the cls token.\n",
        "        N = shape_list(self.positional_embedding)[1] - 1\n",
        "        \n",
        "        # Segregate the cls embedding from the position embeddings.\n",
        "        class_pos_embed = self.positional_embedding[:, 0]\n",
        "        patch_pos_embed = self.positional_embedding[:, 1:]\n",
        "        dim = shape_list(x)[-1]\n",
        "\n",
        "        pos_embed = tf.cond(\n",
        "            tf.logical_and(tf.equal(num_patches, N), tf.equal(h, w)),\n",
        "            lambda: self.positional_embedding,\n",
        "            lambda: self.interpolate_pos(x, N, h, w)\n",
        "        )\n",
        "        return pos_embed\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        n, h, w, c = shape_list(inputs)\n",
        "\n",
        "        # Create patches and project the patches.\n",
        "        projected_patches = self.projection(inputs)\n",
        "\n",
        "        # Append class token.\n",
        "        cls_token = tf.tile(self.cls_token, (n, 1, 1))\n",
        "        if cls_token.dtype != projected_patches.dtype:\n",
        "            cls_token = tf.cast(cls_token, projected_patches.dtype)\n",
        "        projected_patches = tf.concat([cls_token, projected_patches], axis=1)\n",
        "\n",
        "        # Fetch positional embeddings.\n",
        "        positional_embedding = self.interpolate_pos_embedding(\n",
        "            projected_patches, h, w\n",
        "        )\n",
        "\n",
        "        # Add positional embeddings to the projected patches.\n",
        "        encoded_patches = (\n",
        "            positional_embedding + projected_patches\n",
        "        )  # (B, number_patches, projection_dim)\n",
        "        encoded_patches = self.dropout(encoded_patches)\n",
        "\n",
        "        # Initialize a dictionary to store attention scores from each transformer\n",
        "        # block.\n",
        "        attention_scores = dict()\n",
        "\n",
        "        # Iterate over the number of layers and stack up blocks of\n",
        "        # Transformer.\n",
        "        for transformer_module in self.transformer_blocks:\n",
        "            # Add a Transformer block.\n",
        "            encoded_patches, attention_score = transformer_module(encoded_patches)\n",
        "            attention_scores[f\"{transformer_module.name}_att\"] = attention_score\n",
        "\n",
        "        # Final layer normalization.\n",
        "        representation = self.layer_norm(encoded_patches)\n",
        "\n",
        "        # Pool representation.\n",
        "        # Reference: https://github.com/facebookresearch/dino/blob/main/eval_linear.py#L259-#L260\n",
        "        encoded_patches = representation[:, 0]\n",
        "        encoded_patches_exp = tf.expand_dims(encoded_patches, -1)\n",
        "        avg_patch_tokens = tf.reduce_mean(representation[:, 1:], 1)\n",
        "        avg_patch_tokens = tf.expand_dims(avg_patch_tokens, -1)\n",
        "        output = tf.concat([encoded_patches_exp, avg_patch_tokens], -1)\n",
        "        output = tf.reshape(output, (n, -1))\n",
        "\n",
        "        # Classification head.\n",
        "        output = self.head(output)\n",
        "\n",
        "        if training:\n",
        "            return output\n",
        "        else:\n",
        "            return output, attention_scores"
      ],
      "metadata": {
        "id": "E3HNIzlyeREm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validating the initial architecture"
      ],
      "metadata": {
        "id": "AvbwLEshmXZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = base_config.get_config(model_name=\"vit_base\", projection_dim=768, num_heads=12)\n",
        "\n",
        "vit_dino_base = ViTDINOBase(config)\n",
        "\n",
        "dummy_inputs = tf.random.normal((2, 224, 224, 3))\n",
        "outputs, attn_scores = vit_dino_base(dummy_inputs)\n",
        "\n",
        "keys = list(attn_scores.keys())\n",
        "print(attn_scores[keys[-1]].shape)"
      ],
      "metadata": {
        "id": "-1F3jn02ego4",
        "outputId": "90ad85a0-fa66-4200-9d67-9a96462273b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 12, 197, 197)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Port pre-trained DINO params\n",
        "\n",
        "Reference: https://github.com/sayakpaul/deit-tf/"
      ],
      "metadata": {
        "id": "oREXDqYDme9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pt_model_dict = {k: backbone_state_dict[k].numpy() for k in backbone_state_dict}"
      ],
      "metadata": {
        "id": "cXk3tOlci9TH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_dino_base.layers[0].layers[0] = helpers.modify_tf_block(\n",
        "    vit_dino_base.layers[0].layers[0],\n",
        "    pt_model_dict[\"patch_embed.proj.weight\"],\n",
        "    pt_model_dict[\"patch_embed.proj.bias\"],\n",
        ")\n",
        "\n",
        "# Positional embedding.\n",
        "vit_dino_base.positional_embedding.assign(tf.Variable(pt_model_dict[\"pos_embed\"]))\n",
        "\n",
        "# CLS and (optional) Distillation tokens.\n",
        "# Distillation token won't be present in the models trained without distillation.\n",
        "vit_dino_base.cls_token.assign(tf.Variable(pt_model_dict[\"cls_token\"]))\n",
        "\n",
        "# Layer norm layers.\n",
        "ln_idx = -2\n",
        "vit_dino_base.layers[ln_idx] = helpers.modify_tf_block(\n",
        "    vit_dino_base.layers[ln_idx],\n",
        "    pt_model_dict[\"norm.weight\"],\n",
        "    pt_model_dict[\"norm.bias\"],\n",
        ")\n",
        "\n",
        "# Head layers.\n",
        "head_layer = vit_dino_base.get_layer(\"classification_head\")\n",
        "vit_dino_base.layers[-1] = helpers.modify_tf_block(\n",
        "    head_layer,\n",
        "    pt_model_dict[\"head.weight\"],\n",
        "    pt_model_dict[\"head.bias\"],\n",
        ")\n",
        "\n",
        "# Transformer blocks.\n",
        "idx = 0\n",
        "\n",
        "for outer_layer in vit_dino_base.layers:\n",
        "    if isinstance(outer_layer, tf.keras.Model) and outer_layer.name != \"projection\":\n",
        "        tf_block = vit_dino_base.get_layer(outer_layer.name)\n",
        "        pt_block_name = f\"blocks.{idx}\"\n",
        "\n",
        "        # LayerNorm layers.\n",
        "        layer_norm_idx = 1\n",
        "        for layer in tf_block.layers:\n",
        "            if isinstance(layer, tf.keras.layers.LayerNormalization):\n",
        "                layer_norm_pt_prefix = f\"{pt_block_name}.norm{layer_norm_idx}\"\n",
        "                layer.gamma.assign(\n",
        "                    tf.Variable(pt_model_dict[f\"{layer_norm_pt_prefix}.weight\"])\n",
        "                )\n",
        "                layer.beta.assign(\n",
        "                    tf.Variable(pt_model_dict[f\"{layer_norm_pt_prefix}.bias\"])\n",
        "                )\n",
        "                layer_norm_idx += 1\n",
        "\n",
        "        # FFN layers.\n",
        "        ffn_layer_idx = 1\n",
        "        for layer in tf_block.layers:\n",
        "            if isinstance(layer, tf.keras.layers.Dense):\n",
        "                dense_layer_pt_prefix = f\"{pt_block_name}.mlp.fc{ffn_layer_idx}\"\n",
        "                layer = helpers.modify_tf_block(\n",
        "                    layer,\n",
        "                    pt_model_dict[f\"{dense_layer_pt_prefix}.weight\"],\n",
        "                    pt_model_dict[f\"{dense_layer_pt_prefix}.bias\"],\n",
        "                )\n",
        "                ffn_layer_idx += 1\n",
        "\n",
        "        # Attention layer.\n",
        "        for layer in tf_block.layers:\n",
        "            (q_w, k_w, v_w), (q_b, k_b, v_b) = helpers.get_tf_qkv(\n",
        "                f\"{pt_block_name}.attn\",\n",
        "                pt_model_dict,\n",
        "                config,\n",
        "            )\n",
        "\n",
        "            if isinstance(layer, mha.TFViTAttention):\n",
        "                # Key\n",
        "                layer.self_attention.key = helpers.modify_tf_block(\n",
        "                    layer.self_attention.key,\n",
        "                    k_w,\n",
        "                    k_b,\n",
        "                    is_attn=True,\n",
        "                )\n",
        "                # Query\n",
        "                layer.self_attention.query = helpers.modify_tf_block(\n",
        "                    layer.self_attention.query,\n",
        "                    q_w,\n",
        "                    q_b,\n",
        "                    is_attn=True,\n",
        "                )\n",
        "                # Value\n",
        "                layer.self_attention.value = helpers.modify_tf_block(\n",
        "                    layer.self_attention.value,\n",
        "                    v_w,\n",
        "                    v_b,\n",
        "                    is_attn=True,\n",
        "                )\n",
        "                # Final dense projection\n",
        "                layer.dense_output.dense = helpers.modify_tf_block(\n",
        "                    layer.dense_output.dense,\n",
        "                    pt_model_dict[f\"{pt_block_name}.attn.proj.weight\"],\n",
        "                    pt_model_dict[f\"{pt_block_name}.attn.proj.bias\"],\n",
        "                )\n",
        "\n",
        "        idx += 1"
      ],
      "metadata": {
        "id": "EFIi9GeRjA9n"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video generation for attention maps\n",
        "\n",
        "Code copied and modified from the [official code](https://github.com/facebookresearch/dino/blob/main/video_generation.py). "
      ],
      "metadata": {
        "id": "ECLhHQaZJ3j8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import cv2\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "r8HINxrJfrY-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FOURCC = {\n",
        "    \"mp4\": cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
        "    \"avi\": cv2.VideoWriter_fourcc(*\"XVID\"),\n",
        "}"
      ],
      "metadata": {
        "id": "NvTo_f6bgIs3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoGeneratorTF:\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "        # For DeiT, DINO this should be unchanged. For the original ViT-B16 models,\n",
        "        # input images should be scaled to [-1, 1] range.\n",
        "        self.norm_layer = keras.layers.Normalization(\n",
        "            mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
        "            variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
        "        )\n",
        "\n",
        "    def run(self):\n",
        "        if self.args.input_path is None:\n",
        "            print(f\"Provided input path {self.args.input_path} is non valid.\")\n",
        "            sys.exit(1)\n",
        "        else:\n",
        "            if self.args.video_only:\n",
        "                self._generate_video_from_images(\n",
        "                    self.args.input_path, self.args.output_path\n",
        "                )\n",
        "            else:\n",
        "                # If input path exists\n",
        "                if os.path.exists(self.args.input_path):\n",
        "                    # If input is a video file\n",
        "                    if os.path.isfile(self.args.input_path):\n",
        "                        frames_folder = os.path.join(self.args.output_path, \"frames-tf\")\n",
        "                        attention_folder = os.path.join(\n",
        "                            self.args.output_path, \"attention-tf\"\n",
        "                        )\n",
        "\n",
        "                        os.makedirs(frames_folder, exist_ok=True)\n",
        "                        os.makedirs(attention_folder, exist_ok=True)\n",
        "\n",
        "                        self._extract_frames_from_video(\n",
        "                            self.args.input_path, frames_folder\n",
        "                        )\n",
        "\n",
        "                        self._inference(\n",
        "                            frames_folder,\n",
        "                            attention_folder,\n",
        "                        )\n",
        "\n",
        "                        self._generate_video_from_images(\n",
        "                            attention_folder, self.args.output_path\n",
        "                        )\n",
        "\n",
        "                    # If input is a folder of already extracted frames\n",
        "                    if os.path.isdir(self.args.input_path):\n",
        "                        attention_folder = os.path.join(\n",
        "                            self.args.output_path, \"attention-tf\"\n",
        "                        )\n",
        "\n",
        "                        os.makedirs(attention_folder, exist_ok=True)\n",
        "\n",
        "                        self._inference(self.args.input_path, attention_folder)\n",
        "\n",
        "                        self._generate_video_from_images(\n",
        "                            attention_folder, self.args.output_path\n",
        "                        )\n",
        "\n",
        "                # If input path doesn't exists\n",
        "                else:\n",
        "                    print(f\"Provided input path {self.args.input_path} doesn't exists.\")\n",
        "                    sys.exit(1)\n",
        "\n",
        "    def _extract_frames_from_video(self, inp: str, out: str):\n",
        "        vidcap = cv2.VideoCapture(inp)\n",
        "        self.args.fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        print(f\"Video: {inp} ({self.args.fps} fps)\")\n",
        "        print(f\"Extracting frames to {out}\")\n",
        "\n",
        "        success, image = vidcap.read()\n",
        "        count = 0\n",
        "        while success:\n",
        "            cv2.imwrite(\n",
        "                os.path.join(out, f\"frame-{count:04}.jpg\"),\n",
        "                image,\n",
        "            )\n",
        "            success, image = vidcap.read()\n",
        "            count += 1\n",
        "\n",
        "    def _generate_video_from_images(self, inp: str, out: str):\n",
        "        img_array = []\n",
        "        attention_images_list = sorted(glob.glob(os.path.join(inp, \"attn-*.jpg\")))\n",
        "\n",
        "        # Get size of the first image\n",
        "        with open(attention_images_list[0], \"rb\") as f:\n",
        "            img = Image.open(f)\n",
        "            img = img.convert(\"RGB\")\n",
        "            size = (img.width, img.height)\n",
        "            img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        print(f\"Generating video {size} to {out}\")\n",
        "\n",
        "        for filename in tqdm(attention_images_list[1:]):\n",
        "            with open(filename, \"rb\") as f:\n",
        "                img = Image.open(f)\n",
        "                img = img.convert(\"RGB\")\n",
        "                img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        out = cv2.VideoWriter(\n",
        "            os.path.join(out, \"video-tf.\" + self.args.video_format),\n",
        "            FOURCC[self.args.video_format],\n",
        "            self.args.fps,\n",
        "            size,\n",
        "        )\n",
        "\n",
        "        for i in range(len(img_array)):\n",
        "            out.write(img_array[i])\n",
        "        out.release()\n",
        "        print(\"Done\")\n",
        "\n",
        "    def _preprocess_image(self, image: Image, size: int):\n",
        "        # Reference: https://www.tensorflow.org/lite/examples/style_transfer/overview\n",
        "        image = np.array(image)\n",
        "        image_resized = tf.expand_dims(image, 0)\n",
        "        shape = tf.cast(tf.shape(image_resized)[1:-1], tf.float32)\n",
        "        short_dim = min(shape)\n",
        "        scale = size / short_dim\n",
        "        new_shape = tf.cast(shape * scale, tf.int32)\n",
        "        image_resized = tf.image.resize(\n",
        "            image_resized, new_shape, \n",
        "        )\n",
        "        return self.norm_layer(image_resized).numpy()\n",
        "\n",
        "    def _inference(self, inp: str, out: str):\n",
        "        print(f\"Generating attention images to {out}\")\n",
        "\n",
        "        for img_path in tqdm(sorted(glob.glob(os.path.join(inp, \"*.jpg\")))):\n",
        "            with open(img_path, \"rb\") as f:\n",
        "                img = Image.open(f)\n",
        "                img = img.convert(\"RGB\")\n",
        "\n",
        "            preprocessed_image = self._preprocess_image(\n",
        "                img, self.args.resize\n",
        "            )\n",
        "            h, w = (\n",
        "                preprocessed_image.shape[1] - preprocessed_image.shape[1] % self.args.patch_size,\n",
        "                preprocessed_image.shape[2] - preprocessed_image.shape[2] % self.args.patch_size,\n",
        "            )\n",
        "            preprocessed_image = preprocessed_image[:, :h, :w, :]\n",
        "\n",
        "            h_featmap = preprocessed_image.shape[1] // self.args.patch_size\n",
        "            w_featmap = preprocessed_image.shape[2] // self.args.patch_size\n",
        "\n",
        "            # Grab the attention scores from the final transformer block.\n",
        "            logits, attention_score_dict = self.args.model(preprocessed_image, training=False)\n",
        "            attentions = attention_score_dict[\"transformer_block_11_att\"].numpy()\n",
        "\n",
        "            nh = attentions.shape[1]  # number of head\n",
        "\n",
        "            # we keep only the output patch attention\n",
        "            attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
        "            attentions = attentions.reshape(nh, h_featmap, w_featmap)\n",
        "            attentions = attentions.transpose((1, 2, 0))\n",
        "\n",
        "            # interpolate\n",
        "            attentions = tf.image.resize(\n",
        "                attentions,\n",
        "                size=(h_featmap*self.args.patch_size, w_featmap*self.args.patch_size)\n",
        "            )\n",
        "            # attentions = tf.keras.layers.UpSampling2D(size=self.args.patch_size, data_format=\"channels_first\")(\n",
        "            #     attentions[None, ...]\n",
        "            # )[0].numpy()\n",
        "    \n",
        "            # save attentions heatmaps\n",
        "            fname = os.path.join(out, \"attn-\" + os.path.basename(img_path))\n",
        "            plt.imsave(\n",
        "                fname=fname,\n",
        "                arr=sum(\n",
        "                    attentions[..., i] * 1 / attentions.shape[-1]\n",
        "                    for i in range(attentions.shape[-1])\n",
        "                ),\n",
        "                cmap=\"inferno\",\n",
        "                format=\"jpg\",\n",
        "            )"
      ],
      "metadata": {
        "id": "aTclj98bezH_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a demo video.\n",
        "!gdown --id 12KScLSdZS5gNvLqoZBenbYeTPaVx4wMj\n",
        "!gdown --id 1dnPP0QvJ2944GaSE47yMgrt3T0yO4R_R"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBHeN4ALnfwj",
        "outputId": "f7684433-9d80-4887-cc8e-d45259f54bb5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12KScLSdZS5gNvLqoZBenbYeTPaVx4wMj\n",
            "To: /content/dog.mp4\n",
            "100% 12.8M/12.8M [00:00<00:00, 44.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dnPP0QvJ2944GaSE47yMgrt3T0yO4R_R\n",
            "To: /content/dino.mp4\n",
            "100% 2.18M/2.18M [00:00<00:00, 123MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = ml_collections.ConfigDict()\n",
        "\n",
        "args.model = vit_dino_base\n",
        "args.patch_size = 16\n",
        "args.input_path = \"dino.mp4\"\n",
        "args.output_path = \"./\"\n",
        "args.resize = 512\n",
        "args.video_only = False\n",
        "args.fps = 30.0\n",
        "args.video_format = \"mp4\""
      ],
      "metadata": {
        "id": "1uiL940Smz2D"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vg = VideoGeneratorTF(args)\n",
        "vg.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx-fQQiTnmBV",
        "outputId": "d99e6d2c-e256-4f88-8b67-79e5d6e9d44a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video: dino.mp4 (25.0 fps)\n",
            "Extracting frames to ./frames-tf\n",
            "Generating attention images to ./attention-tf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 278/278 [01:35<00:00,  2.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating video (896, 512) to ./\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 277/277 [00:01<00:00, 152.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "dino-attention-maps-video.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}